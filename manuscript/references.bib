@Article{wallach2018,
  title = {Reproducible Research Practices, Transparency, and Open Access Data in the Biomedical Literature, 2015\textendash{}2017},
  volume = {16},
  issn = {1544-9173},
  abstract = {Currently, there is a growing interest in ensuring the transparency and reproducibility of the published scientific literature. According to a previous evaluation of 441 biomedical journals articles published in 2000\textendash{}2014, the biomedical literature largely lacked transparency in important dimensions. Here, we surveyed a random sample of 149 biomedical articles published between 2015 and 2017 and determined the proportion reporting sources of public and/or private funding and conflicts of interests, sharing protocols and raw data, and undergoing rigorous independent replication and reproducibility checks. We also investigated what can be learned about reproducibility and transparency indicators from open access data provided on PubMed. The majority of the 149 studies disclosed some information regarding funding (103, 69.1\% [95\% confidence interval, 61.0\% to 76.3\%]) or conflicts of interest (97, 65.1\% [56.8\% to 72.6\%]). Among the 104 articles with empirical data in which protocols or data sharing would be pertinent, 19 (18.3\% [11.6\% to 27.3\%]) discussed publicly available data; only one (1.0\% [0.1\% to 6.0\%]) included a link to a full study protocol. Among the 97 articles in which replication in studies with different data would be pertinent, there were five replication efforts (5.2\% [1.9\% to 12.2\%]). Although clinical trial identification numbers and funding details were often provided on PubMed, only two of the articles without a full text article in PubMed Central that discussed publicly available data at the full text level also contained information related to data sharing on PubMed; none had a conflicts of interest statement on PubMed. Our evaluation suggests that although there have been improvements over the last few years in certain key indicators of reproducibility and transparency, opportunities exist to improve reproducible research practices across the biomedical literature and to make features related to reproducibility more readily visible in PubMed.},
  number = {11},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.2006930},
  author = {Joshua D. Wallach and Kevin W. Boyack and John P. A. Ioannidis},
  year = {2018},
  pages = {e2006930},
  pmid = {30457984},
}
@Article{sison1995,
  title = {Simultaneous Confidence Intervals and Sample Size Determination for Multinomial Proportions},
  volume = {90},
  issn = {0162-1459},
  number = {429},
  journal = {Journal of the American Statistical Association},
  doi = {10.1080/01621459.1995.10476521},
  author = {Cristina P Sison and Joseph Glaz},
  year = {1995},
  pages = {366 369},
}
@Article{womack2015,
  title = {Research {{Data}} in {{Core Journals}} in {{Biology}}, {{Chemistry}}, {{Mathematics}}, and {{Physics}}},
  volume = {10},
  abstract = {This study takes a stratified random sample of articles published in 2014 from the top 10 journals in the disciplines of biology, chemistry, mathematics, and physics, as ranked by impact factor. Sampled articles were examined for their reporting of original data or reuse of prior data, and were coded for whether the data was publicly shared or otherwise made available to readers. Other characteristics such as the sharing of software code used for analysis and use of data citation and DOIs for data were examined. The study finds that data sharing practices are still relatively rare in these disciplines' top journals, but that the disciplines have markedly different practices. Biology top journals share original data at the highest rate, and physics top journals share at the lowest rate. Overall, the study finds that within the top journals, only 13\% of articles with original data published in 2014 make the data available to others.},
  number = {12},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0143460},
  author = {Ryan P. Womack},
  year = {2015},
  pages = {e0143460},
  pmcid = {PMC4670119},
  pmid = {26636676},
}

@Article{hardwicke2019,
  title = {An Empirical Assessment of Transparency and Reproducibility-Related Research Practices in the Social Sciences (2014-2017)},
  abstract = {Serious concerns about research quality have catalyzed a number of reform initiatives intended to improve transparency and reproducibility and thus facilitate self-correction, increase efficiency, and enhance research credibility. Meta-research has evaluated the merits of some individual initiatives; however, this may not capture broader trends reflecting the cumulative contribution of these efforts. In this study, we manually examined a random sample of 250 articles in order to estimate the prevalence of a range of transparency and reproducibility-related indicators in the social sciences literature published between 2014-2017. Few articles indicated availability of materials (16/151, 11\% [95\% confidence interval, 7\% to 16\%]), protocols (0/156, 0\% [0\% to 1\%]), raw data (11/156, 7\% [2\% to 13\%]), or analysis scripts (2/156, 1\% [0\% to 3\%]), and no studies were pre-registered (0/156, 0\% [0\% to 1\%]). Some articles explicitly disclosed funding sources (or lack of; 74/236, 31\% [25\% to 37\%]) and some declared no conflicts of interest (36/236, 15\% [11\% to 20\%]). Replication studies were rare (2/156, 1\% [0\% to 3\%]). Few studies were included in evidence synthesis via systematic review (17/151, 11\% [7\% to 16\%]) or meta-analysis (2/151, 1\% [0\% to 3\%]). Less than half the articles were publicly available (101/250, 40\% [34\% to 47\%]). Minimal adoption of transparency and reproducibility-related research practices could be undermining the credibility and efficiency of social science research. The present study establishes a baseline that can be revisited in the future to assess progress.},
  doi = {10.31222/osf.io/6uhg5},
  author = {Tom E. Hardwicke and Joshua D Wallach and Mallory Kidwell and Theiss Bendixen and Sophia Cr{\"u}well and John P. A. Ioannidis},
  month = {apr},
  year = {2019},
}
@Article{vazire2018,
  title = {Implications of the {{Credibility Revolution}} for {{Productivity}}, {{Creativity}}, and {{Progress}}},
  volume = {13},
  issn = {1745-6916},
  abstract = {The credibility revolution (sometimes referred to as the ``replicability crisis'') in psychology has brought about many changes in the standards by which psychological science is evaluated. These changes include (a) greater emphasis on transparency and openness, (b) a move toward preregistration of research, (c) more direct-replication studies, and (d) higher standards for the quality and quantity of evidence needed to make strong scientific claims. What are the implications of these changes for productivity, creativity, and progress in psychological science? These questions can and should be studied empirically, and I present my predictions here. The productivity of individual researchers is likely to decline, although some changes (e.g., greater collaboration, data sharing) may mitigate this effect. The effects of these changes on creativity are likely to be mixed: Researchers will be less likely to pursue risky questions; more likely to use a broad range of methods, designs, and populations; and less free to define their own best practices and standards of evidence. Finally, the rate of scientific progress\textemdash{}the most important shared goal of scientists\textemdash{}is likely to increase as a result of these changes, although one's subjective experience of making progress will likely become rarer.},
  number = {4},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691617751884},
  author = {Simine Vazire},
  year = {2018},
  pages = {411-417},
  pmid = {29961410},
}

@Article{nelson2018,
  title = {Psychology's {{Renaissance}}},
  volume = {69},
  issn = {0066-4308},
  abstract = {In 2010\textendash{}2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology's Renaissance. We begin by describing how psychologists' concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically. Expected final online publication date for the Annual Review of Psychology Volume 69 is January 4, 2018. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  number = {1},
  journal = {Annual Review of Psychology},
  doi = {10.1146/annurev-psych-122216-011836},
  author = {Leif D Nelson and Joseph P Simmons and Uri Simonsohn},
  year = {2018},
  pages = {511 534},
  pmid = {29068778},
}
@Article{stodden2016,
  title = {Enhancing Reproducibility for Computational Methods},
  volume = {354},
  issn = {0036-8075},
  number = {6317},
  journal = {Science},
  doi = {10.1126/science.aah6168},
  author = {Victoria Stodden and Marcia McNutt and David H. Bailey and Ewa Deelman and Yolanda Gil and Brooks Hanson and Michael A. Heroux and John P. A. Ioannidis and Michela Taufer},
  year = {2016},
  pages = {1240-1241},
  pmid = {27940837},
}
@InCollection{buckheit1995,
  address = {{New York, NY}},
  title = {{{WaveLab}} and {{Reproducible Research}}},
  volume = {103},
  isbn = {978-0-387-94564-4 978-1-4612-2544-7},
  abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
  language = {en},
  booktitle = {Wavelets and {{Statistics}}},
  publisher = {{Springer New York}},
  doi = {10.1007/978-1-4612-2544-7_5},
  author = {Jonathan B. Buckheit and David L. Donoho},
  editor = {P. Bickel and P. Diggle and S. Fienberg and K. Krickeberg and I. Olkin and N. Wermuth and S. Zeger and Anestis Antoniadis and Georges Oppenheim},
  year = {1995},
  pages = {55-81},
  file = {/Users/tomhardwicke/Zotero/storage/WZK3DTR7/Buckheit and Donoho - 1995 - WaveLab and Reproducible Research.pdf},
}
@Article{klein2018a,
  title = {A {{Practical Guide}} for {{Transparency}} in {{Psychological Science}}},
  volume = {4},
  issn = {2474-7394},
  abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research (e.g., choosing a repository, preparing their research products for sharing, structuring folders, etc.). Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal \textendash{} each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
  number = {1},
  journal = {Collabra: Psychology},
  doi = {10.1525/collabra.158},
  author = {Olivier Klein and Tom E. Hardwicke and Frederik Aust and Johannes Breuer and Henrik Danielsson and Alicia Hofelich Mohr and Hans Ijzerman and Gustav Nilsonne and Wolf Vanpaemel and Michael C. Frank},
  year = {2018},
  pages = {20},
}
@TechReport{cruwell2018,
  type = {Preprint},
  title = {7 {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {7 {{Easy Steps}} to {{Open Science}}},
  abstract = {The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  language = {en},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cfzyx},
  author = {Sophia Cr{\"u}well and Johnny {van Doorn} and Alexander Etz and Matthew C. Makel and Hannah Moshontz and Jesse Niebaum and Amy Orben and Sam Parsons and Michael Schulte-Mecklenbeck},
  month = {nov},
  year = {2018},
  file = {/Users/tomhardwicke/Zotero/storage/ENA9KMC2/Cr√ºwell et al. - 2018 - 7 Easy Steps to Open Science An Annotated Reading.pdf},
}
@Article{young2008,
  title = {Why Current Publication Practices May Distort Science.},
  volume = {5},
  issn = {1549-1277},
  abstract = {John Ioannidis and colleagues argue that the current system of publication in biomedical research provides a distorted view of the reality of scientific data.},
  number = {10},
  journal = {PLoS Medicine},
  doi = {10.1371/journal.pmed.0050201},
  author = {Neal S Young and John P. A. Ioannidis and Omar Al-Ubaydli},
  year = {2008},
  pages = {e201},
  pmid = {18844432},
}
@Article{chalmers2009,
  title = {Viewpoint {{Avoidable}} Waste in the Production and Reporting of Research Evidence},
  volume = {374},
  issn = {0140-6736},
  number = {9683},
  journal = {The Lancet},
  doi = {10.1016/s0140-6736(09)60329-9},
  author = {Iain Chalmers and Paul Glasziou},
  year = {2009},
  pages = {86 89},
  pmid = {19525005},
}
@Article{ioannidis2012,
  title = {Why {{Science Is Not Necessarily Self}}-{{Correcting}}},
  volume = {7},
  issn = {1745-6916},
  abstract = {The ability to self-correct is considered a hallmark of science. However, self-correction does not always happen to scientific evidence by default. The trajectory of scientific credibility can fluctuate over time, both for defined scientific fields and for science at-large. History suggests that major catastrophes in scientific credibility are unfortunately possible and the argument that ``it is obvious that progress is made'' is weak. Careful evaluation of the current status of credibility of various scientific fields is important in order to understand any credibility deficits and how one could obtain and establish more trustworthy results. Efficient and unbiased replication mechanisms are essential for maintaining high levels of scientific credibility. Depending on the types of results obtained in the discovery and replication phases, there are different paradigms of research: optimal, self-correcting, false nonreplication, and perpetuated fallacy. In the absence of replication efforts, one is left with unconfirmed (genuine) discoveries and unchallenged fallacies. In several fields of investigation, including many areas of psychological science, perpetuated and unchallenged fallacies may comprise the majority of the circulating evidence. I catalogue a number of impediments to self-correction that have been empirically studied in psychological science. Finally, I discuss some proposed solutions to promote sound replication practices enhancing the credibility of scientific results as well as some potential disadvantages of each of them. Any deviation from the principle that seeking the truth has priority over any other goals may be seriously damaging to the self-correcting functions of science.},
  number = {6},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691612464056},
  author = {John P. A. Ioannidis},
  year = {2012},
  pages = {645-654},
  pmid = {26168125},
}
@Article{hardwicke2018a,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the {{journalCognition}}},
  volume = {5},
  issn = {2054-5703},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data ('analytic reproducibility'). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  number = {8},
  journal = {Royal Society Open Science},
  doi = {10.1098/rsos.180448},
  author = {Tom E. Hardwicke and Maya B Mathur and Kyle MacDonald and Gustav Nilsonne and George C Banks and Mallory C Kidwell and Alicia Hofelich Mohr and Elizabeth Clayton and Erica J Yoon and Michael Henry Tessler and Richie L Lenne and Sara Altman and Bria Long and Michael C Frank},
  year = {2018},
  pages = {180448},
  pmid = {30225032},
}

@TechReport{obels2019,
  type = {Preprint},
  title = {Analysis of {{Open Data}} and {{Computational Reproducibility}} in {{Registered Reports}} in {{Psychology}}},
  abstract = {Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to re-use or check published research. These benefits will only emerge if researchers can reproduce the analysis reported in published articles, and if data is annotated well enough so that it is clear what all variables mean. Because most researchers have not been trained in computational reproducibility, it is important to evaluate current practices to identify practices that can be improved. We examined data and code sharing, as well as computational reproducibility of the main results, without contacting the original authors, for Registered Reports published in the psychological literature between 2014 and 2018. Of the 62 articles that met our inclusion criteria, data was available for 40 articles, and analysis scripts for 37 articles. For the 35 articles that shared both data and code and performed analyses in SPSS, R, Python, MATLAB, or JASP, we could run the scripts for 31 articles, and reproduce the main results for 20 articles. Although the articles that shared both data and code (35 out of 62, or 56\%) and articles that could be computationally reproduced (20 out of 35, or 57\%) was relatively high compared to other studies, there is clear room for improvement. We provide practical recommendations based on our observations, and link to examples of good research practices in the papers we reproduced.},
  language = {en},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/fk8vh},
  author = {Pepijn Obels and Daniel Lakens and Nicholas Alvaro Coles and Jaroslav Gottfried and Seth Ariel Green},
  month = {may},
  year = {2019},
  file = {/Users/tomhardwicke/Zotero/storage/HWUZ4UJ9/Obels et al. - 2019 - Analysis of Open Data and Computational Reproducib.pdf},
}
@Article{orben2019,
  title = {The Association between Adolescent Well-Being and Digital Technology Use},
  abstract = {The widespread use of digital technologies by young people has spurred speculation that their regular use negatively impacts psychological well-being. Current empirical evidence supporting this idea is largely based on secondary analyses of large-scale social datasets. Though these datasets provide a valuable resource for highly powered investigations, their many variables and observations are often explored with an analytical flexibility that marks small effects as statistically significant, thereby leading to potential false positives and conflicting results. Here we address these methodological challenges by applying specification curve analysis (SCA) across three large-scale social datasets (total n = 355,358) to rigorously examine correlational evidence for the effects of digital technology on adolescents. The association we find between digital technology use and adolescent well-being is negative but small, explaining at most 0.4\% of the variation in well-being. Taking the broader context of the data into account suggests that these effects are too small to warrant policy change. Adolescents regularly use digital technology, but its impact on their psychological well-being is unclear. Here, the authors examine three large datasets and find only a small negative association: digital technology use explains at most 0.4\% of well-being.},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0506-1},
  author = {Amy Orben and Andrew K. Przybylski},
  year = {2019},
  pages = {1-10},
}

@Article{steegen2016,
  title = {Increasing {{Transparency Through}} a {{Multiverse Analysis}}},
  volume = {11},
  issn = {1745-6916},
  abstract = {Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.},
  number = {5},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691616658637},
  author = {S Steegen and F Tuerlinckx and A Gelman and W Vanpaemel},
  year = {2016},
  pages = {702 712},
  pmid = {27694465},
}
@Article{tierney2015,
  title = {Individual {{Participant Data}} ({{IPD}}) {{Meta}}-Analyses of {{Randomised Controlled Trials}}: {{Guidance}} on {{Their Use}}},
  volume = {12},
  issn = {1549-1277},
  abstract = {Jayne Tierney and colleagues offer guidance on how to spot a well-designed and well-conducted individual participant data meta-analysis.},
  number = {7},
  journal = {PLoS Medicine},
  doi = {10.1371/journal.pmed.1001855},
  author = {Jayne F Tierney and Claire Vale and Richard Riley and Catrin Tudur Smith and Lesley Stewart and Mike Clarke and Maroeska Rovers},
  year = {2015},
  pages = {e1001855 16},
  pmcid = {PMC4510878},
  pmid = {26196287},
}
@Article{voytek2016,
  title = {The {{Virtuous Cycle}} of a {{Data Ecosystem}}},
  volume = {12},
  issn = {1553-734X},
  number = {8},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1005037},
  author = {Bradley Voytek},
  year = {2016},
  pages = {e1005037 6},
  pmcid = {PMC4974004},
  pmid = {27490108},
}
@Article{baker2016,
  title = {Is There a Reproducibility Crisis? {{A Nature}} Survey Lifts the Lid on How Researchers View the'crisis Rocking Science and What They Think Will Help},
  volume = {533},
  issn = {0028-0836},
  abstract = {More than 70\% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature's survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research.The data reveal sometimes- contradictory attitudes towards reproducibility. Although 52\% of those surveyed agree that ...},
  number = {7552},
  journal = {Nature},
  doi = {10.1038/521274a},
  author = {M Baker},
  year = {2016},
  pages = {452 454},
  pmid = {25993940},
}

@Article{anderson2010,
  title = {Extending the {{Mertonian Norms}}: {{Scientists}}' {{Subscription}} to {{Norms}} of {{Research}}.},
  volume = {81},
  issn = {1538-4640},
  abstract = {This analysis, based on focus groups and a national survey, assesses scientists' subscription to the Mertonian norms of science and associated counternorms. It also supports extension of these norms to governance (as opposed to administration), as a norm of decision-making, and quality (as opposed to quantity), as a evaluative norm.},
  number = {3},
  journal = {The Journal of higher education},
  doi = {10.1353/jhe.0.0095},
  author = {Melissa S Anderson and Emily A Ronning and Raymond Devries and Brian C Martinson},
  year = {2010},
  pages = {366 393},
  pmid = {21132074},
}
@Article{houtkoop2018,
  title = {Data {{Sharing}} in {{Psychology}}: {{A Survey}} on {{Barriers}} and {{Preconditions}}},
  volume = {1},
  issn = {2515-2459},
  abstract = {Despite its potential to accelerate academic progress in psychological science, public data sharing remains relatively uncommon. In order to discover the perceived barriers to public data sharing and possible means for lowering them, we conducted a survey, which elicited responses from 600 authors of articles in psychology. The results confirmed that data are shared only infrequently. Perceived barriers included respondents' belief that sharing is not a common practice in their fields, their preference to share data only upon request, their perception that sharing requires extra work, and their lack of training in sharing data. Our survey suggests that strong encouragement from institutions, journals, and funders will be particularly effective in overcoming these barriers, in combination with educational materials that demonstrate where and how data can be shared effectively.},
  number = {1},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245917751886},
  author = {Bobby Lee Houtkoop and Chris Chambers and Malcolm Macleod and Dorothy V. M. Bishop and Thomas E. Nichols and Eric-Jan Wagenmakers},
  year = {2018},
  pages = {70-85},
}

@Article{fuchs2012,
  title = {Psychologists {{Are Open}} to {{Change}}, yet {{Wary}} of {{Rules}}},
  volume = {7},
  issn = {1745-6916},
  abstract = {Psychologists must change the way they conduct and report their research\textemdash{}this notion has been the topic of much debate in recent years. One article recently published in Psychological Science proposing six requirements for researchers concerning data collection and reporting practices as well as four guidelines for reviewers aimed at improving the publication process has recently received much attention (Simmons, Nelson, \& Simonsohn, 2011). We surveyed 1,292 psychologists to address two questions: Do psychologists support these concrete changes to data collection, reporting, and publication practices, and if not, what are their reasons? Respondents also indicated the percentage of print and online journal space that should be dedicated to novel studies and direct replications as well as the percentage of published psychological research that they believed would be confirmed if direct replications were conducted. We found that psychologists are generally open to change. Five requirements for researchers and three guidelines for reviewers were supported as standards of good practice, whereas one requirement was even supported as a publication condition. Psychologists appear to be less in favor of mandatory conditions of publication than standards of good practice. We conclude that the proposal made by Simmons, Nelson \& Simonsohn (2011) is a starting point for such standards.},
  number = {6},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691612459521},
  author = {Heather M. Fuchs and Mirjam Jenny and Susann Fiedler},
  year = {2012},
  pages = {639-642},
  pmid = {26168123},
}
@Article{miguel2014,
  title = {Promoting {{Transparency}} in {{Social Science Research}}},
  volume = {343},
  issn = {0036-8075},
  number = {6166},
  journal = {Science},
  doi = {10.1126/science.1245317},
  author = {E. Miguel and C. Camerer and K. Casey and J. Cohen and K. M. Esterling and A. Gerber and R. Glennerster and D. P. Green and M. Humphreys and G. Imbens and D. Laitin and T. Madon and L. Nelson and B. A. Nosek and M. Petersen and R. Sedlmayr and J. P. Simmons and U. Simonsohn and M. Van {der Laan}},
  year = {2014},
  pages = {30-31},
  pmcid = {PMC4103621},
  pmid = {24385620},
}

@Article{nosek2015,
  title = {Promoting an Open Research Culture},
  volume = {348},
  issn = {0036-8075},
  abstract = {Summary Transparency, openness, and reproducibility are readily recognized as vital features of science (1, 2). When asked, most scientists embrace these features as disciplinary norms and values (3). Therefore, one might expect that these valued features},
  number = {6242},
  journal = {Science},
  doi = {10.1126/science.aab2374},
  author = {Brian A Nosek and G Alter and G C Banks and D Borsboom and S D Bowman and S J Breckler and S Buck and Christopher D Chambers and G Chin and G Christensen and M Contestabile and A Dafoe and E Eich and J Freese and R Glennerster and D Goroff and D P Green and B Hesse and M Humphreys and J Ishiyama and D Karlan and A Kraut and A Lupia and T Madon and N Malhotra and E Mayo-Wilson and M McNutt and E Miguel and E L Paluck and Uri Simonsohn and C Soderberg and B A Spellman and J Turitto and G VandenBos and S Vazire and Eric-Jan Wagenmakers and R Wilson and Tal Yarkoni},
  year = {2015},
  pages = {1422 1425},
  pmcid = {PMC4550299},
  pmid = {26113702},
}

@Article{munafo2017,
  title = {A Manifesto for Reproducible Science},
  volume = {1},
  issn = {2397-3374},
  abstract = {\textexclamdown{}p\textquestiondown{}Leading voices in the reproducibility landscape call for the adoption of measures to optimize key elements of the scientific process.\textexclamdown/p\textquestiondown{}},
  number = {1},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-016-0021},
  author = {Marcus R Munaf{\a`o} and Brian A Nosek and Dorothy V M Bishop and Katherine S Button and Christopher D Chambers and Nathalie Percie {du Sert} and Uri Simonsohn and Eric-Jan Wagenmakers and Jennifer J Ware and John P. A. Ioannidis},
  year = {2017},
  pages = {1 9},
}
@Article{hardwicke2019a,
  title = {Calibrating the Scientific Ecosystem through Meta-Research},
  abstract = {Whilst some scientists study insects, molecules, brains, or clouds, other scientists study science itself. Meta-research, or ``research-on-research'', is a burgeoning discipline that investigates efficiency, quality, and bias in the scientific ecosystem, topics that have become especially relevant amid widespread concerns about the credibility of the scientific literature. Meta-research may help calibrate the scientific ecosystem towards higher standards by providing empirical evidence that informs the iterative generation and refinement of reform initiatives. We introduce a translational framework that involves (1) identifying problems; (2) investigating problems; (3) developing solutions; and (4) evaluating solutions. In each of these areas, we review key meta-research endeavors and discuss several examples of prior and ongoing work. The scientific ecosystem is perpetually evolving; the discipline of meta-research presents an opportunity to use empirical evidence to guide its development and maximize its potential.},
  journal = {Annual Review of Statistics and its Application},
  doi = {10.31222/osf.io/krb58},
  author = {Tom E. Hardwicke and Stylianos Serghiou and Perrine Janiaud and Valentin Danchev and Sophia Cr{\"u}well and Steve Goodman and John P. A. Ioannidis},
  year = {2019},
}

@Article{ioannidis2018a,
  title = {Meta-Research: {{Why}} Research on Research Matters},
  volume = {16},
  issn = {1544-9173},
  abstract = {Meta-research is the study of research itself: its methods, reporting, reproducibility, evaluation, and incentives. Given that science is the key driver of human progress, improving the efficiency of scientific investigation and yielding more credible and more useful research results can translate to major benefits. The research enterprise grows very fast. Both new opportunities for knowledge and innovation and new threats to validity and scientific integrity emerge. Old biases abound, and new ones continuously appear as novel disciplines emerge with different standards and challenges. Meta-research uses an interdisciplinary approach to study, promote, and defend robust science. Major disruptions are likely to happen in the way we pursue scientific investigation, and it is important to ensure that these disruptions are evidence based.},
  number = {3},
  journal = {PLoS Biology},
  doi = {10.1371/journal.pbio.2005468},
  author = {John P. A. Ioannidis},
  year = {2018},
  pages = {e2005468 6},
  pmid = {29534060},
}
@Article{rowhani-farid2016,
  title = {Has Open Data Arrived at the {{British Medical Journal}} ({{BMJ}})? {{An}} Observational Study},
  volume = {6},
  issn = {2044-6055},
  abstract = {Objective To quantify data sharing trends and data sharing policy compliance at the British Medical Journal (BMJ) by analysing the rate of data sharing practices, and investigate attitudes and examine barriers towards data sharing. Design Observational study. Setting The BMJ research archive.},
  number = {10},
  journal = {BMJ Open},
  doi = {10.1136/bmjopen-2016-011784},
  author = {Anisa Rowhani-Farid and Adrian G Barnett},
  year = {2016},
  pages = {e011784 9},
  pmid = {27737882},
}
@Article{iqbal2016,
  title = {Reproducible Research Practices and Transparency across the Biomedical Literature},
  volume = {14},
  issn = {1544-9173},
  abstract = {Examination of recent trends in reproducibility and transparency practices in biomedical research reveals an ongoing lack of access to full datasets and detailed protocols for both clinical and non-clinical studies.},
  number = {1},
  journal = {PLoS Biology},
  doi = {10.1371/journal.pbio.1002333},
  author = {Shareen A Iqbal and Joshua D Wallach and Muin J Khoury and Sheri D Schully and John P. A. Ioannidis},
  year = {2016},
  pages = {e1002333 13},
  pmcid = {PMC4699702},
  pmid = {26726926},
}
@Article{szucs2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  volume = {15},
  issn = {1544-9173},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64\textendash{}1.46) for nominally statistically significant results and D = 0.24 (0.11\textendash{}0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  number = {3},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.2000797},
  author = {Denes Szucs and John P. A. Ioannidis},
  year = {2017},
  pages = {e2000797},
  pmid = {28253258},
}

@Article{ioannidis2014b,
  title = {Publication and Other Reporting Biases in Cognitive Sciences: Detection, Prevalence, and Prevention},
  volume = {18},
  issn = {1364-6613},
  abstract = {Trends in Cognitive Sciences, 18 (2014) 236-242. doi:10.1016/j.tics.2014.02.010},
  number = {5},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2014.02.010},
  author = {John P. A. Ioannidis and Marcus R Munaf{\a`o} and Paolo Fusar-Poli and Brian A Nosek and Sean P David},
  year = {2014},
  pages = {235 241},
  pmcid = {PMC4078993},
  pmid = {24656991},
}

@Article{simmons2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  volume = {22},
  issn = {0956-7976},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  number = {11},
  journal = {Psychological science},
  doi = {10.1177/0956797611417632},
  author = {Joseph P Simmons and Leif D Nelson and U Simonsohn},
  year = {2011},
  pages = {1359 1366},
  pmid = {22006061},
}

@Article{ioannidis2005,
  title = {Why Most Published Research Findings Are False},
  volume = {2},
  issn = {1549-1277},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  number = {8},
  journal = {PLoS Medicine},
  doi = {10.1371/journal.pmed.0020124},
  author = {John P. A. Ioannidis},
  year = {2005},
  pages = {e124},
  pmid = {16060722},
}
@Article{john2012,
  title = {Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling},
  volume = {23},
  issn = {0956-7976},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  number = {5},
  journal = {Psychological science},
  doi = {10.1177/0956797611430953},
  author = {L K John and G Loewenstein and D Prelec},
  year = {2012},
  pages = {524 532},
  pmid = {22508865},
}
@Article{vazire2017,
  title = {Quality {{Uncertainty Erodes Trust}} in {{Science}}},
  volume = {3},
  issn = {2474-7394},
  abstract = {When consumers of science (readers and reviewers) lack relevant details about the study design, data, and analyses, they cannot adequately evaluate the strength of a scientific study. Lack of transparency is common in science, and is encouraged by journals that place more emphasis on the aesthetic appeal of a manuscript than the robustness of its scientific claims. In doing this, journals are implicitly encouraging authors to do whatever it takes to obtain eye-catching results. To achieve this, researchers can use common research practices that beautify results at the expense of the robustness of those results (e.g., p-hacking). The problem is not engaging in these practices, but failing to disclose them. A car whose carburetor is duct-taped to the rest of the car might work perfectly fine, but the buyer has a right to know about the duct-taping. Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars \textendash{} they cannot reliably tell the difference between lemons and high quality findings. This phenomenon \textendash{} quality uncertainty \textendash{} has been shown to erode trust in economic markets, such as the used car market. The same problem threatens to erode trust in science. The solution is to increase transparency and give consumers of scientific research the information they need to accurately evaluate research. Transparency would also encourage researchers to be more careful in how they conduct their studies and write up their results. To make this happen, we must tie journals' reputations to their practices regarding transparency. Reviewers hold a great deal of power to make this happen, by demanding the transparency needed to rigorously evaluate scientific manuscripts. The public expects transparency from science, and appropriately so \textendash{} we should be held to a higher standard than used car salespeople.},
  number = {1},
  journal = {Collabra: Psychology},
  doi = {10.1525/collabra.74},
  author = {Simine Vazire},
  year = {2017},
  pages = {1},
  file = {/Users/tomhardwicke/Zotero/storage/5KGW5BSD/Vazire - Quality Uncertainty Erodes Trust in Science.pdf},
}
@Article{ioannidis2014a,
  title = {Increasing Value and Reducing Waste in Research Design, Conduct, and Analysis},
  volume = {383},
  issn = {0140-6736},
  abstract = {Correctable weaknesses in the design, conduct, and analysis of biomedical and public health research studies can produce misleading results and waste valuable resources. Small effects can be difficult to distinguish from bias introduced by study design and analyses. An absence of detailed written protocols and poor documentation of research is common. Information obtained might not be useful or important, and statistical precision or power is often too low or used in a misleading way. Insufficient consideration might be given to both previous and continuing studies. Arbitrary choice of analyses and an overemphasis on random extremes might affect the reported findings. Several problems relate to the research workforce, including failure to involve experienced statisticians and methodologists, failure to train clinical researchers and laboratory scientists in research methods and design, and the involvement of stakeholders with conflicts of interest. Inadequate emphasis is placed on recording of research decisions and on reproducibility of research. Finally, reward systems incentivise quantity more than quality, and novelty more than reliability. We propose potential solutions for these problems, including improvements in protocols and documentation, consideration of evidence from studies in progress, standardisation of research efforts, optimisation and training of an experienced and non-conflicted scientific workforce, and reconsideration of scientific reward systems.},
  number = {9912},
  journal = {The Lancet},
  doi = {10.1016/s0140-6736(13)62227-8},
  author = {John P. A. Ioannidis and Sander Greenland and Mark A Hlatky and Muin J Khoury and Malcolm R Macleod and David Moher and Kenneth F Schulz and Robert Tibshirani},
  year = {2014},
  pages = {166 175},
  pmcid = {PMC4697939},
  pmid = {24411645},
}
@Article{pashler2012a,
  title = {Editors' {{Introduction}} to the {{Special Section}} on {{Replicability}} in {{Psychological Science}}},
  volume = {7},
  issn = {1745-6916},
  number = {6},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691612465253},
  author = {Harold Pashler and Eric-Jan Wagenmakers},
  year = {2012},
  pages = {528-530},
  pmid = {26168108},
}

@Article{hardwicke2018,
  title = {Mapping the Universe of Registered Reports},
  volume = {2},
  abstract = {Registered reports present a substantial departure from traditional publishing models with the goal of enhancing the transparency and credibility of the scientific literature. We map the evolving universe of registered reports to assess their growth, implementation and shortcomings at journals across scientific disciplines.},
  number = {11},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0444-y},
  author = {Tom E. Hardwicke and John P. A. Ioannidis},
  year = {2018},
  pages = {793-796},
}
@Article{piwowar2018,
  title = {The State of {{OA}}: A Large-Scale Analysis of the Prevalence and Impact of {{Open Access}} Articles},
  volume = {6},
  abstract = {Despite growing interest in Open Access (OA) to scholarly literature, there is an unmet need for large-scale, up-to-date, and reproducible studies assessing the prevalence and characteristics of OA. We address this need using oaDOI, an open online service that determines OA status for 67 million articles. We use three samples, each of 100,000 articles, to investigate OA in three populations: (1) all journal articles assigned a Crossref DOI, (2) recent journal articles indexed in Web of Science, and (3) articles viewed by users of Unpaywall, an open-source browser extension that lets users find OA articles using oaDOI. We estimate that at least 28\% of the scholarly literature is OA (19M in total) and that this proportion is growing, driven particularly by growth in Gold and Hybrid. The most recent year analyzed (2015) also has the highest percentage of OA (45\%). Because of this growth, and the fact that readers disproportionately access newer articles, we find that Unpaywall users encounter OA quite frequently: 47\% of articles they view are OA. Notably, the most common mechanism for OA is not Gold, Green, or Hybrid OA, but rather an under-discussed category we dub Bronze: articles made free-to-read on the publisher website, without an explicit Open license. We also examine the citation impact of OA articles, corroborating the so-called open-access citation advantage: accounting for age and discipline, OA articles receive 18\% more citations than average, an effect driven primarily by Green and Hybrid OA. We encourage further research using the free oaDOI service, as a way to inform OA policy and practice.},
  journal = {PeerJ},
  doi = {10.7717/peerj.4375},
  author = {Heather Piwowar and Jason Priem and Vincent Larivi{\a`e}re and Juan Pablo Alperin and Lisa Matthias and Bree Norlander and Ashley Farley and Jevin West and Stefanie Haustein},
  year = {2018},
  pages = {e4375},
  pmid = {29456894},
}
@Article{cristea2018,
  title = {Improving {{Disclosure}} of {{Financial Conflicts}} of {{Interest}} for {{Research}} on {{Psychosocial Interventions}}},
  issn = {2168-622X},
  journal = {JAMA Psychiatry},
  doi = {10.1001/jamapsychiatry.2018.0382},
  author = {Ioana-Alina Cristea and John P. A. Ioannidis},
  year = {2018},
  pmid = {29641818},
}
@Article{bekelman2003,
  title = {Scope and {{Impact}} of {{Financial Conflicts}} of {{Interest}} in {{Biomedical Research}}: {{A Systematic Review}}},
  volume = {289},
  issn = {0098-7484},
  shorttitle = {Scope and {{Impact}} of {{Financial Conflicts}} of {{Interest}} in {{Biomedical Research}}},
  language = {en},
  number = {4},
  journal = {JAMA},
  doi = {10.1001/jama.289.4.454},
  author = {Justin E. Bekelman and Yan Li and Cary P. Gross},
  month = {jan},
  year = {2003},
  pages = {454},
  file = {/Users/tomhardwicke/Zotero/storage/MTIP9QM7/Bekelman et al. - 2003 - Scope and Impact of Financial Conflicts of Interes.pdf},
}
@Article{david2008,
  title = {The {{Historical Origins}} of '{{Open Science}}': {{An Essay}} on {{Patronage}}, {{Reputation}} and {{Common Agency Contracting}} in the {{Scientific Revolution}}},
  volume = {3},
  issn = {1932-0213},
  shorttitle = {The {{Historical Origins}} of '{{Open Science}}'},
  abstract = {This essay examines the economics of patronage in the production of knowledge and its influence upon the historical formation of key elements in the ethos and organizational structure of publicly funded `open science.' The emergence during the late sixteenth and early seventeenth centuries of the idea and practice of `open science' was a distinctive and vital organizational aspect of the Scientific Revolution. It represented a break from the previously dominant ethos of secrecy in the pursuit of Nature's Secrets, to a new set of norms, incentives, and organizational structures that reinforced scientific researchers' commitments to rapid disclosure of new knowledge. The rise of `cooperative rivalries' in the revelation of new knowledge, is seen as a functional response to heightened asymmetric information problems posed for the Renaissance system of court-patronage of the arts and sciences; pre-existing informational asymmetries had been exacerbated by the claims of mathematicians and the increasing practical reliance upon new mathematical techniques in a variety of `contexts of application.' Reputational competition among Europe's noble patrons motivated much of their efforts to attract to their courts the most prestigious natural philosophers, was no less crucial in the workings of that system than was the concern among their would-be clients to raise their peer-based reputational status. In late Renaissance Europe, the feudal legacy of fragmented political authority had resulted in relations between noble patrons and their savantclients that resembled the situation modern economists describe as `common agency contracting in substitutes' -- competition among incompletely informed principals for the dedicated services of multiple agents. These conditions tended to result in contract terms (especially with regard to autonomy and financial support) that left agent client members of the nascent scientific communities better positioned to retain larger information rents on their specialized knowledge. This encouraged entry into their emerging disciplines, and enabled them collectively to develop a stronger degree of professional autonomy for their programs of inquiry within the increasingly specialized and formal scientific academies (such the Acad{\'e}mie royale des Sciences and the Royal Society) that had attracted the patronage of rival absolutist States of Western Europe during the latter part of the seventeenth century. The institutionalization of `open science' that took place within those settings is shown to have continuities with the use by scientists of the earlier humanist academies, and with the logic of regal patronage, rather than being driven by the material requirements of new observational and experimental techniques.},
  language = {en},
  number = {2},
  journal = {Capitalism and Society},
  doi = {10.2202/1932-0213.1040},
  author = {Paul A. David},
  month = {jan},
  year = {2008},
  file = {/Users/tomhardwicke/Zotero/storage/EZG5AXAT/David - 2008 - The Historical Origins of 'Open Science' An Essay.pdf},
}
@Article{kidwell2016,
  title = {Badges to {{Acknowledge Open Practices}}: {{A Simple}}, {{Low}}-{{Cost}}, {{Effective Method}} for {{Increasing Transparency}}},
  volume = {14},
  issn = {1544-9173},
  abstract = {Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3\% of Psychological Science articles reported open data. After badges, 23\% reported open data, with an accelerating trend; 39\% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.},
  number = {5},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.1002456},
  author = {Mallory C. Kidwell and Ljiljana B. Lazarevi{\a'c} and Erica Baranski and Tom E. Hardwicke and Sarah Piechowski and Lina-Sophia Falkenberg and Curtis Kennett and Agnieszka Slowik and Carina Sonnleitner and Chelsey Hess-Holden and Timothy M. Errington and Susann Fiedler and Brian A. Nosek},
  year = {2016},
  pages = {e1002456},
  pmcid = {PMC4865119},
  pmid = {27171007},
}
@Article{alsheikh-ali2011,
  title = {Public Availability of Published Research Data in High-Impact Journals},
  volume = {6},
  abstract = {There is increasing interest to make primary data from published research publicly available. We aimed to assess the current status of making research data available in highly-cited journals across the scientific literature. We reviewed the first 10 original research papers of 2009 published in the 50 original research journals with the highest impact factor. For each journal we documented the policies related to public availability and sharing of data. Of the 50 journals, 44 (88\%) had a statement in their instructions to authors related to public availability and sharing of data. However, there was wide variation in journal requirements, ranging from requiring the sharing of all primary data related to the research to just including a statement in the published manuscript that data can be available on request. Of the 500 assessed papers, 149 (30\%) were not subject to any data availability policy. Of the remaining 351 papers that were covered by some data availability policy, 208 papers (59\%) did not fully adhere to the data availability instructions of the journals they were published in, most commonly (73\%) by not publicly depositing microarray data. The other 143 papers that adhered to the data availability instructions did so by publicly depositing only the specific data type as required, making a statement of willingness to share, or actually sharing all the primary data. Overall, only 47 papers (9\%) deposited full primary raw data online. None of the 149 papers not subject to data availability policies made their full primary data publicly available. A substantial proportion of original research papers published in high-impact journals are either not subject to any data availability policies, or do not adhere to the data availability instructions in their respective journals. This empiric evaluation highlights opportunities for improvement.},
  number = {9},
  journal = {PLoS ONE},
  doi = {10.1371/journal.pone.0024357},
  author = {Alawi A Alsheikh-Ali and Waqas Qureshi and Mouaz H Al-Mallah and John P. A. Ioannidis},
  year = {2011},
  pages = {e24357 4},
  pmcid = {PMC3168487},
  pmid = {21915316},
}

@Article{necker2014,
  title = {Scientific Misbehavior in Economics},
  volume = {43},
  issn = {0048-7333},
  abstract = {This study reports the results of a survey of professional, mostly academic economists about their research norms and scientific misbehavior. Behavior such as data fabrication or plagiarism are (almost) unanimously rejected and admitted by less than 4\% of participants. Research practices that are often considered ``questionable,'' e.g., strategic behavior while analyzing results or in the publication process, are rejected by at least 60\%. Despite their low justifiability, these behaviors are widespread. Ninety-four percent report having engaged in at least one unaccepted research practice. Surveyed economists perceive strong pressure to publish. The level of justifiability assigned to different misdemeanors does not increase with the perception of pressure. However, perceived pressure is found to be positively related to the admission of being involved in several unaccepted research practices. Although the results cannot prove causality, they are consistent with the notion that the ``publish or perish'' culture motivates researchers to violate research norms.},
  number = {10},
  journal = {Research Policy},
  doi = {10.1016/j.respol.2014.05.002},
  author = {Sarah Necker},
  year = {2014},
  pages = {1747-1759},
}
@Article{nutu2019,
  title = {Open Science Practices in Clinical Psychology Journals: {{An}} Audit Study},
  volume = {128},
  issn = {1939-1846(Electronic),0021-843X(Print)},
  shorttitle = {Open Science Practices in Clinical Psychology Journals},
  abstract = {We conducted an audit of 60 clinical psychology journals, covering the first 2 quartiles by impact factor on Web of Science. We evaluated editorial policies in 5 domains crucial to reproducibility and transparency (prospective registration, data sharing, preprints, endorsement of reporting guidelines and conflict of interest [COI] disclosure). We examined implementation in a randomly selected cross-sectional sample of 201 articles published in 2017 in the ``best practice'' journals, defined as having explicit supportive policies in 4 out of 5 domains. Our findings showed that 15 journals cited prospective registration, 40 data sharing, 15 explicitly permitted preprints, 28 endorsed reporting guidelines, and 52 had mandatory policies for COI disclosure. Except for COI disclosure, few policies were mandatory: registration in 15 journals, data sharing in 1, and reporting guidelines for randomized trials in 18 and for meta-analyses in 15. Seventeen journals were identified as ``best practice.'' An analysis of recent articles showed extremely low compliance for prospective registration (3\% articles) and data sharing (2\%). One preprint could be identified. Reporting guidelines were endorsed in 19\% of the articles, though for most articles this domain was rated as nonapplicable. Only half of the articles included a COI disclosure. Desired open science policies should become clear and mandatory, and their enforcement streamlined by reducing the multiplicity of guidelines and templates. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  number = {6},
  journal = {Journal of Abnormal Psychology},
  doi = {10.1037/abn0000414},
  author = {Daria Nutu and Claudio Gentili and Florian Naudet and Ioana A. Cristea},
  year = {2019},
  keywords = {Best Practices,Clinical Psychology,Conflict of Interest,Data Sharing,Policy Making,Professional Standards,Sciences,Scientific Communication},
  pages = {510-516},
  file = {/Users/tomhardwicke/Zotero/storage/IV99PZ46/Nutu et al. - 2019 - Open science practices in clinical psychology jour.pdf;/Users/tomhardwicke/Zotero/storage/GLJ4DDYX/2019-43757-005.html},
}
@Article{evangelou2005,
  title = {Unavailability of Online Supplementary Scientific Information from Articles Published in Major Journals},
  volume = {19},
  issn = {0892-6638},
  abstract = {Printed articles increasingly rely on online supplements to store critical scientific information, but such data may eventually become unavailable. We checked the current availability of online supplementary scientific information published in six top-cited scientific journals (Science, Nature, Cell, New England Journal of Medicine, Lancet, Proceedings of the National Academy of Sciences USA). Here we show that in 4.7\% and 9.6\% of articles with online supplementary material, some of the supplements became unavailable within 2 and 5 years of their publication, respectively.\textemdash{}Evangelou, E., Trikalinos, T. A., Ioannidis, J. P. A. Unavailability of online supplementary scientific information from articles published in major journals.},
  number = {14},
  journal = {The FASEB Journal},
  doi = {10.1096/fj.05-4784lsf},
  author = {Evangelos Evangelou and Thomas A. Trikalinos and John P. A. Ioannidis},
  year = {2005},
  pages = {1943-1944},
  pmid = {16319137},
}
@Article{bourne2017,
  title = {Ten Simple Rules to Consider Regarding Preprint Submission},
  volume = {13},
  issn = {1553-734X},
  number = {5},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1005473},
  author = {Philip E Bourne and Jessica K Polka and Ronald D Vale and Robert Kiley},
  year = {2017},
  pages = {e1005473 6},
  pmid = {28472041},
}
@Article{simons2014,
  title = {The {{Value}} of {{Direct Replication}}},
  volume = {9},
  issn = {1745-6916},
  abstract = {Reproducibility is the cornerstone of science. If an effect is reliable, any competent researcher should be able to obtain it when using the same procedures with adequate statistical power. Two of the articles in this special section question the value of direct replication by other laboratories. In this commentary, I discuss the problematic implications of some of their assumptions and argue that direct replication by multiple laboratories is the only way to verify the reliability of an effect.},
  number = {1},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691613514755},
  author = {Daniel J. Simons},
  year = {2014},
  pages = {76-80},
  pmid = {26173243},
}
@Article{rowhani-farid2018,
  title = {Badges for Sharing Data and Code at {{Biostatistics}}: An Observational Study},
  volume = {7},
  issn = {2046-1402},
  shorttitle = {Badges for Sharing Data and Code at {{Biostatistics}}},
  abstract = {Background
              :~The reproducibility policy at the journal~
              Biostatistics
              ~rewards articles with badges for data and code sharing.~ This study investigates the effect of badges at increasing reproducible research.


              Methods
              :~ The setting of this observational study is the~
              Biostatistics~
              and
              ~Statistics in Medicine
              (control journal) online research archives.~ The data consisted of 240 randomly sampled articles from 2006 to 2013 (30 articles per year) per journal.~ Data analyses included: plotting probability of data and code sharing by article submission date, and Bayesian logistic regression modelling.


              Results
              :~ The probability of data sharing was higher at~
              Biostatistics~
              than the control journal but the probability of code sharing was comparable for both journals.~ The probability of data sharing increased by 3.9 times (95\% credible interval: 1.5 to 8.44 times, p-value probability that sharing increased: 0.998) after badges were introduced at~
              Biostatistics
              .~ On an absolute scale, this difference was only a 7.6\% increase in data sharing (95\% CI: 2 to 15\%, p-value: 0.998).~ Badges did not have an impact on code sharing at the journal (mean increase: 1 time, 95\% credible interval: 0.03 to 3.58 times, p-value probability that sharing increased: 0.378).~ 64\% of articles at
              Biostatistics
              that provide data/code had broken links, and at
              Statistics in Medicine
              , 40\%; assuming these links worked only slightly changed the effect of badges on data (mean increase: 6.7\%, 95\% CI: 0.0\% to 17.0\%, p-value: 0.974) and on code (mean increase: -2\%, 95\% CI: -10.0 to 7.0\%, p-value: 0.286).


              Conclusions:
              ~ The effect of badges at~
              Biostatistics
              ~was a 7.6\% increase in the data sharing rate, 5 times less than the effect of badges at~
              Psychological Science
              .~ Though badges at~
              Biostatistics
              ~did not impact code sharing, and had a moderate effect on data sharing, badges are an interesting step that journals are taking to incentivise and promote reproducible research.},
  language = {en},
  journal = {F1000Research},
  doi = {10.12688/f1000research.13477.2},
  author = {Anisa Rowhani-Farid and Adrian G. Barnett},
  month = {mar},
  year = {2018},
  pages = {90},
  file = {/Users/tomhardwicke/Zotero/storage/WI4YX9T2/Rowhani-Farid and Barnett - 2018 - Badges for sharing data and code at Biostatistics.pdf},
}
@Article{vines2014,
  title = {The Availability of Research Data Declines Rapidly with Article Age},
  volume = {24},
  issn = {0960-9822},
  abstract = {Current Biology, 24 (2014) 94-97. doi:10.1016/j.cub.2013.11.014},
  number = {1},
  journal = {Current Biology},
  doi = {10.1016/j.cub.2013.11.014},
  author = {Timothy H Vines and Arianne Y K Albert and Rose L Andrew and Florence D{\a'e}barre and Dan G Bock and Michelle T Franklin and Kimberly J Gilbert and Jean-S{\a'e}bastien Moore and S{\a'e}bastien Renaut and Diana J Rennison},
  year = {2014},
  pages = {94 97},
  eprint = {1312.5670},
  pmid = {24361065},
}
@Article{vanpaemel2015,
  title = {Are We Wasting a Good Crisis? {{The}} Availability of Psychological Research Data after the Storm},
  volume = {1},
  issn = {2376-6832},
  abstract = {2015. Collabra, 1(1): 3, pp. 1-5, DOI: http://dx.doi.org/10.1525/collabra.13},
  number = {1},
  journal = {Collabra},
  doi = {10.1525/collabra.13},
  author = {Wolf Vanpaemel and Maarten Vermorgen and Leen Deriemaecker and Gert Storms},
  year = {2015},
  pages = {1 5},
}

@Article{wicherts2006,
  title = {The Poor Availability of Psychological Research Data for Reanalysis.},
  volume = {61},
  issn = {0003-066X},
  abstract = {The origin of the present comment lies in a failed attempt to obtain, through e-mailed requests, data reported in 141 empirical articles recently published by the American Psychological Association (APA). Our original aim was to reanalyze these data sets to assess the robustness of the research findings to outliers. We never got that far. In June 2005, we contacted the corresponding author of every article that appeared in the last two 2004 issues of four major APA journals. Because their articles had been published in APA journals, we were certain that all of the authors had signed the APA Certification of Compliance With APA Ethical Principles, which includes the principle on sharing data for reanalysis. Unfortunately, 6 months later, after writing more than 400 e-mails\textendash{}and sending some corresponding authors detailed descriptions of our study aims, approvals of our ethical committee, signed assurances not to share data with others, and even our full resumes-we ended up with a meager 38 positive reactions and the actual data sets from 64 studies (25.7\% of the total number of 249 data sets). This means that 73\% of the authors did not share their data.},
  number = {7},
  journal = {American Psychologist},
  doi = {10.1037/0003-066x.61.7.726},
  author = {Jelte M Wicherts and Denny Borsboom and Judith Kats and Dylan Molenaar},
  year = {2006},
  pages = {726 728},
  pmid = {17032082},
}
@Article{hardwicke2018b,
  title = {Populating the {{Data Ark}}: {{An}} Attempt to Retrieve, Preserve, and Liberate Data from the Most Highly-Cited Psychology and Psychiatry Articles},
  volume = {13},
  abstract = {The vast majority of scientific articles published to-date have not been accompanied by concomitant publication of the underlying research data upon which they are based. This state of affairs precludes the routine re-use and re-analysis of research data, undermining the efficiency of the scientific enterprise, and compromising the credibility of claims that cannot be independently verified. It may be especially important to make data available for the most influential studies that have provided a foundation for subsequent research and theory development. Therefore, we launched an initiative\textemdash{}the Data Ark\textemdash{}to examine whether we could retrospectively enhance the preservation and accessibility of important scientific data. Here we report the outcome of our efforts to retrieve, preserve, and liberate data from 111 of the most highly-cited articles published in psychology and psychiatry between 2006\textendash{}2011 (n = 48) and 2014\textendash{}2016 (n = 63). Most data sets were not made available (76/111, 68\%, 95\% CI [60, 77]), some were only made available with restrictions (20/111, 18\%, 95\% CI [10, 27]), and few were made available in a completely unrestricted form (15/111, 14\%, 95\% CI [5, 22]). Where extant data sharing systems were in place, they usually (17/22, 77\%, 95\% CI [54, 91]) did not allow unrestricted access. Authors reported several barriers to data sharing, including issues related to data ownership and ethical concerns. The Data Ark initiative could help preserve and liberate important scientific data, surface barriers to data sharing, and advance community discussions on data stewardship.},
  number = {8},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0201856},
  author = {Tom E. Hardwicke and John P. A. Ioannidis},
  year = {2018},
  pages = {e0201856},
  pmid = {30071110},
}
@Article{meyer2017,
  title = {Practical {{Tips}} for {{Ethical Data Sharing}}},
  volume = {1},
  issn = {2515-2459},
  abstract = {This Tutorial provides practical dos and don'ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say\textemdash{}and what not to say\textemdash{}in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing ``public'' data.},
  number = {1},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245917747656},
  author = {Michelle N. Meyer},
  year = {2017},
  pages = {131-144},
}
@Article{lebel2018,
  title = {A {{Unified Framework}} to {{Quantify}} the {{Credibility}} of {{Scientific Findings}}},
  volume = {1},
  issn = {2515-2459},
  abstract = {Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. Published research, however, can be useful for theory or application only if it is credible. In science, a credible finding is one that has repeatedly survived risky falsification attempts. However, state-of-the-art meta-analytic approaches cannot determine the credibility of an effect because they do not account for the extent to which each included study has survived such attempted falsification. To overcome this problem, we outline a unified framework for estimating the credibility of published research by examining four fundamental falsifiability-related dimensions: (a) transparency of the methods and data, (b) reproducibility of the results when the same data-processing and analytic decisions are reapplied, (c) robustness of the results to different data-processing and analytic decisions, and (d) replicability of the effect. This framework includes a standardized workflow in which the degree to which a finding has survived scrutiny is quantified along these four facets of credibility. The framework is demonstrated by applying it to published replications in the psychology literature. Finally, we outline a Web implementation of the framework and conclude by encouraging the community of researchers to contribute to the development and crowdsourcing of this platform.},
  number = {3},
  journal = {Advances in Methods and Practices in Psychological Science},
  doi = {10.1177/2515245918787489},
  author = {Etienne P. LeBel and Randy J. McCarthy and Brian D. Earp and Malte Elson and Wolf Vanpaemel},
  year = {2018},
  pages = {389-402},
}
@Article{morey2016,
  title = {The {{Peer Reviewers}}' {{Openness Initiative}}: Incentivizing Open Research Practices through Peer Review},
  volume = {3},
  abstract = {Openness is one of the central values of science. Open scientific practices such as sharing data, materials and analysis scripts alongside published articles have many benefits, including easier replication and extension studies, increased availability of data for theory-building and meta-analysis, and increased possibility of review and collaboration even after a paper has been published. Although modern information technology makes sharing easier than ever before, uptake of open practices had been slow. We suggest this might be in part due to a social dilemma arising from misaligned incentives and propose a specific, concrete mechanism-reviewers withholding comprehensive review-to achieve the goal of creating the expectation of open practices as a matter of scientific principle.},
  number = {1},
  journal = {Royal Society Open Science},
  doi = {10.1098/rsos.150547},
  author = {Richard D Morey and Christopher D Chambers and Peter J Etchells and Christine R Harris and Rink Hoekstra and Dani{\"e}l Lakens and Stephan Lewandowsky and Candice Coker Morey and Daniel P Newman and Felix D Sch{\"o}nbrodt and Wolf Vanpaemel and Eric-Jan Wagenmakers and Rolf A Zwaan},
  year = {2016},
  pages = {150547},
  pmcid = {PMC4736937},
  pmid = {26909182},
}
@Article{stodden2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  volume = {115},
  issn = {0027-8424},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy\textemdash{}author remission of data and code postpublication upon request\textemdash{}an improvement over no policy, but currently insufficient for reproducibility.},
  number = {11},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1708290115},
  author = {Victoria Stodden and Jennifer Seiler and Zhaokun Ma},
  year = {2018},
  pages = {201708290},
  pmid = {29531050},
}
@Article{wilson2017,
  title = {Good Enough Practices in Scientific Computing},
  volume = {13},
  issn = {1553-7358},
  language = {en},
  number = {6},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1005510},
  author = {Greg Wilson and Jennifer Bryan and Karen Cranston and Justin Kitzes and Lex Nederbragt and Tracy K. Teal},
  editor = {Francis Ouellette},
  month = {jun},
  year = {2017},
  pages = {e1005510},
  file = {/Users/tomhardwicke/Zotero/storage/MP7NJUYE/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/Users/tomhardwicke/Zotero/storage/XXLJNSSN/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf},
}
@TechReport{clyburne-sherin2018,
  type = {Preprint},
  title = {Computational {{Reproducibility}} via {{Containers}} in {{Psychology}}},
  abstract = {NOTE: Accepted in principle at Meta-Psychology, submission number MP2018.892, link: https://osf.io/ps5ru/. Anyone can participate in peer review by sending the editor an email, or through discussion on social media. The preferred way of open commenting, however, is to use the hypothes.is integration at PsyArXiv and directly comment on this preprint. Editor: Rickard Carlsson, rickard.carlsson@lnu.seWebsite: https://open.lnu.se/index.php/metapsychology ABSTRACT: Scientific progress relies on the replication and reuse of research. However, despite an emerging culture of sharing code and data in psychology, the research practices needed to achieve computational reproducibility -- the quality of a research project entailing the provision of sufficient code, data and documentation to allow an independent researcher to re-obtain the project's results -- are not widely adopted. Historically, the ability to share and reuse computationally reproducible research was technically challenging and time-consuming. One welcome development on this front is the advent of containers, a technology intended to facilitate code sharing for software development. Containers, however, remain technically demanding and imperfectly suited for research applications. This editorial argues that the use of containers adapted for research can help foster a culture of reproducibiliy in psychology research. We will illustrate this by introducing Code Ocean, an online computational reproducibility platform. (Disclaimer: the authors work for Code Ocean.)},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/mf82t},
  author = {April Clyburne-Sherin and Xu Fei and Seth Ariel Green},
  month = {feb},
  year = {2018},
}
@Article{kimmelman2014,
  title = {Distinguishing between {{Exploratory}} and {{Confirmatory Preclinical Research Will Improve Translation}}},
  volume = {12},
  issn = {1544-9173},
  abstract = {Preclinical researchers confront two overarching agendas related to drug development: selecting interventions amid a vast field of candidates, and producing rigorous evidence of clinical promise for a small number of interventions. We suggest that each challenge is best met by two different, complementary modes of investigation. In the first (exploratory investigation), researchers should aim at generating robust pathophysiological theories of disease. In the second (confirmatory investigation), researchers should aim at demonstrating strong and reproducible treatment effects in relevant animal models. Each mode entails different study designs, confronts different validity threats, and supports different kinds of inferences. Research policies should seek to disentangle the two modes and leverage their complementarity. In particular, policies should discourage the common use of exploratory studies to support confirmatory inferences, promote a greater volume of confirmatory investigation, and customize design and reporting guidelines for each mode.},
  number = {5},
  journal = {PLoS Biology},
  doi = {10.1371/journal.pbio.1001863},
  author = {Jonathan Kimmelman and Jeffrey S Mogil and Ulrich Dirnagl},
  year = {2014},
  pages = {e1001863 4},
  pmcid = {PMC4028181},
  pmid = {24844265},
}

@Article{wagenmakers2012,
  title = {An {{Agenda}} for {{Purely Confirmatory Research}}},
  volume = {7},
  issn = {1745-6916},
  abstract = {The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology's academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result\textemdash{}a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label ``confirmatory,'' and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled ``exploratory.'' We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.},
  number = {6},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691612463078},
  author = {Eric-Jan Wagenmakers and Ruud Wetzels and Denny Borsboom and Han L. J. {van der Maas} and Rogier A. Kievit},
  year = {2012},
  pages = {632-638},
  pmid = {26168122},
}
@Article{nosek2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  issn = {1364-6613},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2019.07.009},
  author = {Brian A. Nosek and Emorie D. Beck and Lorne Campbell and Jessica K. Flake and Tom E. Hardwicke and David T. Mellor and Anna E. {van 't Veer} and Simine Vazire},
  year = {2019},
}

@Article{nosek2018,
  title = {The Preregistration Revolution},
  volume = {115},
  issn = {0027-8424},
  abstract = {\textexclamdown{}p\textquestiondown{}Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes\textemdash{}a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.\textexclamdown/p\textquestiondown{}},
  number = {11},
  journal = {Proceedings of the National Academy of Sciences of The United States of America},
  doi = {10.1073/pnas.1708274114},
  author = {Brian A Nosek and Charles R Ebersole and Alexander C DeHaven and David T Mellor},
  year = {2018},
  pages = {2600 2606},
  pmid = {29531091},
}
@Article{dickersin2012,
  title = {The {{Evolution}} of {{Trial Registries}} and {{Their Use}} to {{Assess}} the {{Clinical Trial Enterprise}}},
  volume = {307},
  issn = {0098-7484},
  number = {17},
  journal = {JAMA},
  doi = {10.1001/jama.2012.4230},
  author = {Kay Dickersin and Drummond Rennie},
  year = {2012},
  pages = {1861-1864},
  pmid = {22550202},
}
@Article{franco2016,
  title = {Underreporting in {{Psychology Experiments}}},
  volume = {7},
  issn = {1948-5506},
  abstract = {Many scholars have raised concerns about the credibility of empirical findings in psychology, arguing that the proportion of false positives reported in the published literature dramatically exceeds the rate implied by standard significance levels. A major contributor of false positives is the practice of reporting a subset of the potentially relevant statistical analyses pertaining to a research project. This study is the first to provide direct evidence of selective underreporting in psychology experiments. To overcome the problem that the complete experimental design and full set of measured variables are not accessible for most published research, we identify a population of published psychology experiments from a competitive grant program for which questionnaires and data are made publicly available because of an institutional rule. We find that about 40\% of studies fail to fully report all experimental conditions and about 70\% of studies do not report all outcome variables included in the questionnaire. Reported effect sizes are about twice as large as unreported effect sizes and are about 3 times more likely to be statistically significant.},
  number = {1},
  journal = {Social Psychological and Personality Science},
  doi = {10.1177/1948550615598377},
  author = {Annie Franco and Neil Malhotra and Gabor Simonovits},
  year = {2016},
  pages = {8-12},
}
@Article{makel2012,
  title = {Replications in {{Psychology Research}}},
  volume = {7},
  issn = {1745-6916},
  abstract = {Recent controversies in psychology have spurred conversations about the nature and quality of psychological research. One topic receiving substantial attention is the role of replication in psychological science. Using the complete publication history of the 100 psychology journals with the highest 5-year impact factors, the current article provides an overview of replications in psychological research since 1900. This investigation revealed that roughly 1.6\% of all psychology publications used the term replication in text. A more thorough analysis of 500 randomly selected articles revealed that only 68\% of articles using the term replication were actual replications, resulting in an overall replication rate of 1.07\%. Contrary to previous findings in other fields, this study found that the majority of replications in psychology journals reported similar findings to their original studies (i.e., they were successful replications). However, replications were significantly less likely to be successful when there was no overlap in authorship between the original and replicating articles. Moreover, despite numerous systemic biases, the rate at which replications are being published has increased in recent decades.},
  number = {6},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691612460688},
  author = {Matthew C. Makel and Jonathan A. Plucker and Boyd Hegarty},
  year = {2012},
  pages = {537-542},
  pmid = {26168110},
}
@Article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  volume = {349},
  issn = {0036-8075},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  number = {6251},
  journal = {Science},
  doi = {10.1126/science.aac4716},
  author = "{Open Science Collaboration}",
  year = {2015},
  pages = {aac4716},
  pmid = {26315443},
}
@TechReport{claesen2019,
  type = {Preprint},
  title = {Preregistration: {{Comparing Dream}} to {{Reality}}},
  shorttitle = {Preregistration},
  abstract = {Doing research inevitably involves making numerous decisions that can influence research outcomes in such a way that it leads to overconfidence in statistical conclusions. One proposed method to increase the interpretability of a research finding is preregistration, which involves documenting analytic choices on a public, third-party repository prior to any influence by data. To investigate whether, in psychology, preregistration lives up to that potential, we focused on all articles published in Psychological Science with a preregistered badge between February 2015 and November 2017, and assessed the adherence to their corresponding preregistration plans. We observed deviations from the plan in all studies, and, more importantly, in all but one study, at least one of these deviations was not fully disclosed. We discuss examples and possible explanations, and highlight good practices for preregistering research.},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/d8wex},
  author = {Aline Claesen and Sara Lucia Brazuna Tavares Gomes and Francis Tuerlinckx and wolf vanpaemel},
  month = {may},
  year = {2019},
}

@TechReport{veldkamp2018,
  type = {Preprint},
  title = {Ensuring the Quality and Specificity of Preregistrations},
  abstract = {Researchers face many, often seemingly arbitrary choices in formulating hypotheses, designing protocols, collecting data, analyzing data, and reporting results. Opportunistic use of `researcher degrees of freedom' aimed at obtaining statistical significance increases the likelihood of obtaining false positive results and overestimated effect sizes, and lowers the replicability of published results. Preregistration is a mechanism for reducing such degrees of freedom by specifying designs and analysis plans before observing the research outcomes.  The effectiveness of preregistration may depend, in part, on whether the process facilitates sufficiently specific articulation of such plans.  We compared two formats of preregistration available on the OSF: Standard Pre-Data Collection Registration and Prereg Challenge registration.  The Prereg Challenge format provides detailed instructions, a guided workflow, and an independent review to confirm completeness; the ``Standard'' format has minimal direct guidance to give researchers flexibility for what to pre-specify. Results of comparing random samples of 53 preregistrations from each format indicate that neither format restricted all researcher degrees of freedom and the Prereg Challenge format performed better on restricting degrees of freedom than the ``Standard'' format.  Preregistering research is an acquired skill, and registration formats that provide effective guidance will improve the quality of research.},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/cdgyh},
  author = {Coosje Lisabet Sterre Veldkamp and Marjan Bakker and Marcel A. L. M. {van Assen} and Elise Anne Victoire Crompvoets and How Hwee Ong and Brian A. Nosek and Courtney K. Soderberg and David Thomas Mellor and Jelte M. Wicherts},
  month = {sep},
  year = {2018},
}
@Article{naudet2018,
  title = {Data Sharing and Reanalysis of Randomized Controlled Trials in Leading Biomedical Journals with a Full Data Sharing Policy: Survey of Studies Published in {{The BMJ}} and {{PLOS Medicine}}},
  volume = {360},
  issn = {0959-8138},
  abstract = {Objectives To explore the effectiveness of data sharing by randomized controlled trials (RCTs) in journals with a full data sharing policy and to describe potential difficulties encountered in the process of performing reanalyses of the primary outcomes.Design Survey of published RCTs.Setting PubMed/Medline.},
  journal = {BMJ},
  doi = {10.1136/bmj.k400},
  author = {Florian Naudet and Charlotte Sakarovitch and Perrine Janiaud and Ioana Cristea and Daniele Fanelli and David Moher and John P A Ioannidis},
  year = {2018},
  pages = {k400},
  pmid = {29440066},
}
@Article{zarin2017,
  title = {Update on Trial Registration 11 {{Years}} after the {{ICMJE}} Policy Was Established.},
  volume = {376},
  issn = {0028-4793},
  number = {4},
  journal = {The New England journal of medicine},
  doi = {10.1056/nejmsr1601330},
  author = {Deborah A Zarin and Tony Tse and Rebecca J Williams and Thiyagu Rajakannan},
  year = {2017},
  pages = {383 391},
  file = {/Users/tomhardwicke/Zotero/storage/B4F22AEH/Zarin et al. - 2017 - Update on trial registration 11 Years after the IC.pdf},
  pmid = {28121511},
}
@Article{ioannidis2016b,
  title = {The {{Mass Production}} of {{Redundant}}, {{Misleading}}, and {{Conflicted Systematic Reviews}} and {{Meta}}-Analyses: {{Mass Production}} of {{Systematic Reviews}} and {{Meta}}-Analyses},
  volume = {94},
  issn = {0887378X},
  shorttitle = {The {{Mass Production}} of {{Redundant}}, {{Misleading}}, and {{Conflicted Systematic Reviews}} and {{Meta}}-Analyses},
  language = {en},
  number = {3},
  journal = {The Milbank Quarterly},
  doi = {10.1111/1468-0009.12210},
  author = {John P.A. Ioannidis},
  month = {sep},
  year = {2016},
  pages = {485-514},
  file = {/Users/tomhardwicke/Zotero/storage/H45W8GRS/Ioannidis - 2016 - The Mass Production of Redundant, Misleading, and .pdf},
}
@Article{moher2018,
  title = {Assessing Scientists for Hiring, Promotion, and Tenure},
  volume = {16},
  issn = {1544-9173},
  abstract = {Assessment of researchers is necessary for decisions of hiring, promotion, and tenure. A burgeoning number of scientific leaders believe the current system of faculty incentives and rewards is misaligned with the needs of society and disconnected from the evidence about the causes of the reproducibility crisis and suboptimal quality of the scientific publication record. To address this issue, particularly for the clinical and life sciences, we convened a 22-member expert panel workshop in Washington, DC, in January 2017. Twenty-two academic leaders, funders, and scientists participated in the meeting. As background for the meeting, we completed a selective literature review of 22 key documents critiquing the current incentive system. From each document, we extracted how the authors perceived the problems of assessing science and scientists, the unintended consequences of maintaining the status quo for assessing scientists, and details of their proposed solutions. The resulting table was used as a seed for participant discussion. This resulted in six principles for assessing scientists and associated research and policy implications. We hope the content of this paper will serve as a basis for establishing best practices and redesigning the current approaches to assessing scientists by the many players involved in that process.},
  number = {3},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.2004089},
  author = {David Moher and Florian Naudet and Ioana A. Cristea and Frank Miedema and John P. A. Ioannidis and Steven N. Goodman},
  year = {2018},
  pages = {e2004089},
  pmid = {29596415},
}
@Article{wallach2018a,
  title = {Research, Regulatory, and Clinical Decision-Making: The Importance of Scientific Integrity},
  volume = {93},
  issn = {08954356},
  shorttitle = {Research, Regulatory, and Clinical Decision-Making},
  language = {en},
  journal = {Journal of Clinical Epidemiology},
  doi = {10.1016/j.jclinepi.2017.08.021},
  author = {Joshua D. Wallach and Gregg S. Gonsalves and Joseph S. Ross},
  month = {jan},
  year = {2018},
  pages = {88-93},
  file = {/Users/tomhardwicke/Zotero/storage/KHTUIVN5/Wallach et al. - 2018 - Research, regulatory, and clinical decision-making.pdf},
}
@Article{chivers2019,
  title = {Does Psychology Have a Conflict-of-Interest Problem?},
  volume = {571},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7763},
  journal = {Nature},
  doi = {10.1038/d41586-019-02041-5},
  author = {Tom Chivers},
  month = {jul},
  year = {2019},
  pages = {20-23},
  file = {/Users/tomhardwicke/Zotero/storage/ADZCNUR9/Chivers - 2019 - Does psychology have a conflict-of-interest proble.pdf},
}
@Article{hardwicke2020c,
  title = {Analytic Reproducibility in Articles Receiving Open Data Badges at {{Psychological Science}}: {{An}} Observational Study},
  shorttitle = {Analytic Reproducibility in Articles Receiving Open Data Badges at {{Psychological Science}}},
  author = {Tom E. Hardwicke and Manuel Bohn and Kyle MacDonald and Emily Hembacher and Mich{\a`e}le B. Nuijten and Benjamin Peloquin and Benjamin deMayo and Bria Long and Erica J. Yoon and Michael C. Frank},
  year = {2020},
  month = {jul},
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/h35wt},
  abstract = {For any scientific report, repeating the original analyses upon the original data should yield the original outcomes. We evaluated analytic reproducibility in 25 Psychological Science articles awarded open data badges between 2014-2015. Initially, 16 (64\%, 95\% confidence interval [43,81]) articles contained at least one ``major numerical discrepancy'' ({$>$}10\% difference) prompting us to request input from original authors. Ultimately, target values were reproducible without author involvement for 9 (36\% [20,59]) articles; reproducible with author involvement for 6 (24\% [8,47]) articles; not fully reproducible with no substantive author response for 3 (12\% [0,35]) articles; and not fully reproducible despite author involvement for 7 (28\% [12,51]) articles. Overall, 37 major numerical discrepancies remained out of 789 checked values (5\% [3,6]), but original conclusions did not appear affected. Non-reproducibility was primarily caused by unclear reporting of analytic procedures. These results highlight that open data alone is not sufficient to ensure analytic reproducibility.},
  file = {/Users/tomhardwicke/Zotero/storage/ZERUGGIP/Hardwicke et al. - 2020 - Analytic reproducibility in articles receiving ope.pdf;/Users/tomhardwicke/Zotero/storage/ALT9S5LR/h35wt.html},
}
@Article{hardwicke2020,
  title = {An Empirical Assessment of Transparency and Reproducibility-Related Research Practices in the Social Sciences (2014\textendash 2017)},
  author = {Tom E. Hardwicke and Joshua D. Wallach and Mallory C. Kidwell and Theiss Bendixen and Sophia Cr{\"u}well and John P. A. Ioannidis},
  year = {2020},
  volume = {7},
  pages = {190806},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.190806},
  abstract = {Serious concerns about research quality have catalysed a number of reform initiatives intended to improve transparency and reproducibility and thus facilitate self-correction, increase efficiency and enhance research credibility. Meta-research has evaluated the merits of some individual initiatives; however, this may not capture broader trends reflecting the cumulative contribution of these efforts. In this study, we manually examined a random sample of 250 articles in order to estimate the prevalence of a range of transparency and reproducibility-related indicators in the social sciences literature published between 2014 and 2017. Few articles indicated availability of materials (16/151, 11\% [95\% confidence interval, 7\% to 16\%]), protocols (0/156, 0\% [0\% to 1\%]), raw data (11/156, 7\% [2\% to 13\%]) or analysis scripts (2/156, 1\% [0\% to 3\%]), and no studies were pre-registered (0/156, 0\% [0\% to 1\%]). Some articles explicitly disclosed funding sources (or lack of; 74/236, 31\% [25\% to 37\%]) and some declared no conflicts of interest (36/236, 15\% [11\% to 20\%]). Replication studies were rare (2/156, 1\% [0\% to 3\%]). Few studies were included in evidence synthesis via systematic review (17/151, 11\% [7\% to 16\%]) or meta-analysis (2/151, 1\% [0\% to 3\%]). Less than half the articles were publicly available (101/250, 40\% [34\% to 47\%]). Minimal adoption of transparency and reproducibility-related research practices could be undermining the credibility and efficiency of social science research. The present study establishes a baseline that can be revisited in the future to assess progress.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Hardwicke_etal_2020.pdf;/Users/tomhardwicke/Zotero/storage/ZGJRJUQ3/rsos.html},
  journal = {Royal Society Open Science},
  number = {2},
}
@Article{hardwicke2020b,
  title = {Calibrating the {{Scientific Ecosystem Through Meta}}-{{Research}}},
  author = {Tom E. Hardwicke and Stylianos Serghiou and Perrine Janiaud and Valentin Danchev and Sophia Cr{\"u}well and Steven N. Goodman and John P.A. Ioannidis},
  year = {2020},
  month = {mar},
  volume = {7},
  pages = {11--37},
  publisher = {{Annual Reviews}},
  issn = {2326-8298},
  doi = {10.1146/annurev-statistics-031219-041104},
  abstract = {While some scientists study insects, molecules, brains, or clouds, other scientists study science itself. Meta-research, or research-on-research, is a burgeoning discipline that investigates efficiency, quality, and bias in the scientific ecosystem, topics that have become especially relevant amid widespread concerns about the credibility of the scientific literature. Meta-research may help calibrate the scientific ecosystem toward higher standards by providing empirical evidence that informs the iterative generation and refinement of reform initiatives. We introduce a translational framework that involves (a) identifying problems, (b) investigating problems, (c) developing solutions, and (d) evaluating solutions. In each of these areas, we review key meta-research endeavors and discuss several examples of prior and ongoing work. The scientific ecosystem is perpetually evolving; the discipline of meta-research presents an opportunity to use empirical evidence to guide its development and maximize its potential.},
  file = {/Users/tomhardwicke/pCloud Drive/Zotero_Library/Hardwicke_etal_22.pdf;/Users/tomhardwicke/Zotero/storage/D46KIKI2/annurev-statistics-031219-041104.html},
  journal = {Annual Review of Statistics and Its Application},
  number = {1},
}
@Article{nuijten2017,
  title = {Journal {{Data Sharing Policies}} and {{Statistical Reporting Inconsistencies}} in {{Psychology}}},
  author = {Mich{\a`e}le B. Nuijten and Jeroen Borghuis and Coosje L. S. Veldkamp and Linda Dominguez-Alvarez and Marcel A. L. M. {Van Assen} and Jelte M. Wicherts},
  year = {2017},
  month = {dec},
  volume = {3},
  pages = {31},
  issn = {2474-7394},
  doi = {10.1525/collabra.102},
  file = {/Users/tomhardwicke/Zotero/storage/B43TWINC/Nuijten et al. - 2017 - Journal Data Sharing Policies and Statistical Repo.pdf},
  journal = {Collabra: Psychology},
  language = {en},
  number = {1},
}
