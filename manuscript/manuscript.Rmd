---
title             : "Post-replication citation patterns in psychology: Four case studies"
shorttitle        : "Post-replication citation patterns"

author: 
  - name          : "Tom E. Hardwicke"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Nieuwe Achtergracht 129B, Department of Psychology, University of Amsterdam, 1018 WS Amsterdam, The Netherlands"
    email         : "tom.hardwicke@uva.nl"
  - name          : "Dénes Szűcs"
    affiliation   : "3"
  - name          : "Robert T. Thibault"
    affiliation   : "4,5"
  - name          : "Sophia Crüwell"
    affiliation   : "2"
  - name          : "Olmo R. van den Akker"
    affiliation   : "6"
  - name          : "Michèle B. Nuijten"
    affiliation   : "6"
  - name          : "John P. A. Ioannidis"
    affiliation   : "2,7,8"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, University of Amsterdam"
  - id            : "2"
    institution   : "Meta-Research Innovation Center Berlin (METRIC-B), QUEST Center for Transforming Biomedical Research, Charité – Universitätsmedizin Berlin"
  - id            : "3"
    institution   : "Department of Psychology, University of Cambridge, UK"
  - id            : "4"
    institution   : "School of Psychological Science, University of Bristol"
  - id            : "5"
    institution   : "MRC Integrative Epidemiology Unit at the University of Bristol"
  - id            : "6"
    institution   : "Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University"
  - id            : "7"
    institution   : "Meta-Research Innovation Center at Stanford (METRICS), Stanford University"
  - id            : "8"
    institution   : "Departments of Medicine, of Health Research and Policy, of Biomedical Data Science, and of Statistics, Stanford University"

author_note: >
 
abstract: >
   Replication studies that contradict prior findings may facilitate scientific self-correction by triggering a reappraisal of the original studies; however, the research community's response to replication results has not been studied systematically. One approach for gauging responses to replication results is to examine how they impact citations to original studies. In this study, we explored post-replication citation patterns in the context of four prominent multi-laboratory replication attempts published in the field of psychology that strongly contradicted and outweighed prior findings. Generally, we observed a small post-replication decline in the number of favourable citations and a small increase in unfavourable citations. This indicates only modest corrective effects and implies considerable perpetuation of belief in the original findings. Replication results that strongly contradict an original finding do not necessarily nullify its credibility; however, one might at least expect the replication results to be acknowledged and explicitly debated in subsequent literature. By contrast, we found substantial citation bias: the majority of articles citing the original studies neglected to cite relevant replication results. Of those articles that did cite the replication, but continued to cite the original study favourably, approximately half offered an explicit defence of the original study. Our findings suggest that even replication results that strongly contradict original findings do not necessarily prompt a corrective response from the research community. 
  
keywords          : "Replication, self-correction, citations, citation bias, meta-research"

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "en-US"
class             : "man"

output            : 
  papaja::apa6_pdf:
    includes:
      after_body: "appendix.tex"
---

```{r load_packages, warning=F}
# load packages
library(knitr) # for literate programming
library(papaja) # for article template
library(tidyverse) # for data munging
library(here) # for finding files
library(ggpubr) # for arranging plots
library(kableExtra) # for table formatting
```

```{r perform_preprocessing}
# loads raw data, performs preprocessing, saves preprocessed files
source(here("analysis", "preprocessing.R"))
```

```{r load_data}
# loads the processed data files output by the preprocessing performed above
load(here("data", "processed", "d_citations.rds"))
load(here("data", "processed", "d_reference.rds"))
load(here("data", "processed", "d_contentAnalysis.rds"))
load(here("data", "processed", "d_coauthors.rds"))
```

```{r load_functions}
# load custom functions
source(here("analysis", "functions.R"))
```

```{r add_standardized_citation_counts}
# identify the five different case studies we are looking at
caseNames <- c("baumeister", "sripada", "strack", "carter", "caruso")
replicationYears <- c(2016, 2016, 2016, 2014, 2014)

# set up some colour palettes
teal <- "#00798c"
blue <- "dodgerblue4"
red <- "#CC6677"
yellow <- "#DDCC77"
lightgrey <- "grey75"
darkgrey <- "grey35"
beige <- "#D55E00"

p1 <- c(lightgrey, darkgrey, red, yellow, teal)
p2 <- c(lightgrey, beige, blue)

# summarizes data at the year level for each case and adds standardized citation counts for target citations and reference class citations
d_summary <- data.frame() # create empty list to hold data frames for each case
for (i in seq(1, 5)) { # loop through cases
  d <- standardizeCitations(
    thisCase = caseNames[i],
    citationData = d_citations %>% filter(case == caseNames[i]),
    referenceData = d_reference %>% filter(case == caseNames[i]),
    contentData = d_contentAnalysis %>% filter(case == caseNames[i]),
    replicationYear = replicationYears[i]
  )
  d_summary <- bind_rows(d_summary, d) # append to dataframe
}
```

# Introduction

It is often assumed that science is a self-correcting enterprise: the veracity of scientific knowledge should progressively improve as inaccurate claims are abandoned and accurate claims are reinforced (Vazire & Holcombe, 2020). Replication studies are considered to be a key driver of this process because they may indicate that prior results are exaggerated or erroneous (Ioannidis, 2012; Zwaan et al., 2018). Although interpreting the outcome of replication studies is not necessarily straightforward (Collins, 1985; Earp & Trafimow, 2015; Maxwell et al., 2015), one might expect a replication result that strongly contradicts[^footnote1] and outweighs the results of a prior (‘original’) study to impact how that study is cited in subsequent academic literature. For example, if a replication undermines belief in the credibility of an original finding, one might expect to see a change in the frequency and valence (favourability) of citations to the original study; specifically, a decrease in favourable citations accompanied by an increase in unfavourable citations. However, as discussed below, a variety of interesting patterns could emerge depending on how the research community responds to a replication result. The goal of the present study was to empirically explore and describe post-replication citation patterns in the context of four prominent multi-laboratory replication attempts published in the field of psychology that strongly contradicted and outweighed the findings of prior studies.

[^footnote1]: Determining whether a replication finding contradicts an original finding is a complex issue which we largely side-step here by only examining cases that are strongly contradictory - that is, according to several reasonable quantitative criteria (e.g., effect size magnitude, p-values, Bayes Factors, etc.) the results of the original study and replication study lead to opposing inferences (e.g., absence vs. presence of an effect). Also note that one can accept that results are strongly contradictory and remain agnostic about the explanation for the contradiction.

Table \@ref(tab:patterns) outlines several citation patterns that might follow a contradictory replication result, each reflecting different types of response by the research community. We have tentatively categorised these response patterns as being ‘progressive’ or ‘regressive’ depending on their expected impact on the accumulation of scientific knowledge[^footnote2]. The first set of patterns, belief correction/perpetuation, refer to what is often considered a primary functional role of (contradictory) replication studies - to change belief in the credibility of exaggerated or erroneous original findings (Ioannidis, 2012; Vazire & Holcombe, 2020; Zwaan et al., 2018). In the absence of an explicit defence of an original study that convincingly explains a strongly contradictory replication result (see ‘explicit defence’ below), a progressive response might involve a decrease in favourable citations and an increase in unfavourable citations, reflecting updated beliefs about the credibility of the original finding. Conversely, a regressive response might involve maintenance of (or even increase in) favourable citations, and relatively few unfavourable citations, suggesting a perpetuation of belief in the credibility of the original finding despite the contradictory replication result. Prior research has documented how favourable citations to observational epidemiology studies can persist despite the claims of those studies being strongly contradicted in subsequent randomized trials (Tatsioni et al., 2007). Similarly, it has been reported that even when articles are retracted, they can continue to receive favourable citations (Budd et al., 1998). Thus, there is evidence that belief in the credibility of original findings can perpetuate even when subsequent events cast doubt on their credibility; however, we are unaware of similar evidence in the context of replication studies that were explicitly designed to test the reliability of prior findings.

[^footnote2]: This terminology was inspired by, but does not directly mirror, terminology proposed by Imre Lakatos in his work on the rationality of the research community’s response when a scientific theory is contracted by empirical evidence (Lakatos, 1970).

The second set of patterns in Table \@ref(tab:patterns), citation balance/bias, generally refers to whether positive (supportive) evidence is preferentially cited relative to negative (non-supportive) evidence. This pattern has previously been observed in the context of research on inclusion body myositis; citation content analysis showed that the accumulating literature heavily cited the theory that beta amyloid is involved, ignoring multiple studies that contradicted this theory (Greenberg, 2009; also see Bastiaansen et al., 2015). In the present study, these patterns specifically refer to whether articles citing an original study also cite the subsequent contradictory replication study. A progressive response would be to cite both studies (citation balance), as this involves considering and reporting highly relevant evidence (even if the implications of the replication are disputed, see ‘explicit defence’ below). By contrast, citation bias could occur if articles citing an original study neglect to cite a relevant replication study. Regardless of whether this occurs through lack of awareness or deliberate omission, it can be considered a regressive response because highly relevant evidence is not being reported or considered.

The third set of patterns in Table \@ref(tab:patterns), explicit/absent defence, refers to whether researchers who continue to favourably cite the original finding despite the strongly contradictory replication result offer a concrete defence of the original study. As implied above, even when the result of a replication study strongly contradicts the results of an original study, this does not necessarily nullify the credibility of the original findings; the same criticisms that one might apply to an original study in order to infer that its findings are erroneous or exaggerated may also be applied to replication studies (Collins, 1985; Earp & Trafimow, 2015; Maxwell et al., 2015). Thus, if proponents of the original claim mount an explicit defence that counters the implications of the replication results, this might still be considered a progressive response (though obviously one could disagree with the arguments that are presented). By contrast, if favourable citations to the original study are not accompanied by explicit argumentation about the replication result (an absent defence) this might be considered a regressive response because the replication result is apparently discounted without providing any rationale.

```{r patterns}
tibble(
  "Progressive responses" = c("*Belief correction*
A decrease in the number of favourable citations may reflect a decline in belief in the credibility of the original finding (a belief correction pattern). This may be accompanied by a relative increase in the number of unfavourable citations (an active belief correction pattern) or relatively fewer/no unfavourable citations (a passive belief correction pattern).", "*Citation balance*
Articles that cite the original study may also cite the contradictory replication, reflecting that relevant evidence has been considered (a balanced citation pattern).", "*Explicit defence*
Articles that favourably cite the original study and unfavourably cite the contradictory replication may offer concrete counter-arguments that state why the credibility of the original finding has not been undermined (an explicit defence pattern)."),
  "Regressive responses" = c("*Belief perpetuation*
A maintenance of, or increase in, the number of favourable citations may reflect that a maintenance of or increase in belief in the credibility of the original finding (a belief perpetuation pattern). This may be accompanied by a relative increase in unfavourable citations (a challenged belief perpetuation pattern) or relatively fewer/no unfavourable citations (an unchallenged belief perpetuation pattern).", "*Citation bias*
Articles that cite the original study may neglect to also cite the contradictory replication, reflecting that relevant evidence has been neglected, either through lack of awareness or deliberate omission (a citation bias pattern).", "*Absent defence*
Articles that favourably cite the original study and unfavourably cite the contradictory replication may offer no concrete counter-arguments that state why the credibility of the original finding has not been undermined (an absent defence pattern).")
) %>%
  kable(booktabs = T, longtable = T, caption = "Progressive or regressive responses to strongly contradictory replication results and their expected impact on citation patterns for original studies.") %>%
  column_spec(1, "8.5cm") %>%
  column_spec(2, "8.5cm")
```

In the present study, we explored the post-replication citation patterns described above in the context of four case studies in the field of psychology where the findings of a replication study strongly contradicted and outweighed the findings of an original study (Table \@ref(tab:studyDetails)). In two of the cases, the replication studies were part of a single ‘Many Labs’ project (Klein et al., 2014) and addressed the ‘flag priming effect’ (T. J. Carter et al., 2011) and ‘money priming effect’ (Caruso et al., 2013) respectively. The other two cases involved ‘Registered Replication Reports’ (RRRs; Simons et al., 2014), examining influential demonstrations of the ‘facial feedback effect’ (cf. Strack et al., 1988; Wagenmakers et al., 2016) and the ‘ego-depletion effect’ (cf. Baumeister et al., 1998; Hagger et al., 2016; Sripada et al., 2014) respectively. For methodological reasons (see Hagger et al., 2016), the ego-depletion replication was aimed at a classic study in the field (Baumeister et al., 1998), but actually employed a modified computer-based version of the original paradigm (Sripada et al., 2014). Thus, for this particular case study we examined citation patterns to both of these original studies. We chose these particular case studies because they involved prominent pre-registered multi-laboratory replication attempts with sample sizes between 23 - 211 times larger than the original studies, thus providing highly visible and highly credible evidence that strongly contradicted and outweighed earlier findings[^footnote3]. This facilitates additional interpretative clarity about the citation patterns one might expect to observe.

[^footnote3]: We deliberately focused on a select group of case studies rather than other potentially larger samples in order to aid interpretative clarity; for example, the extent to which the results of the large scale Reproducibility Project in Psychology (RPP; Open Science Collaboration, 2015) actually contradicted the original studies has been contested (Etz & Vandekerckhove, 2016). Additionally, original studies were not actually cited in the RPP research report, which may have diluted awareness about relevant replication studies.

The present study was exploratory in nature and intended to provide descriptive observations rather than test hypotheses. The three sets of expected citation patterns outlined in Table \@ref(tab:patterns) were used to guide our study design and interpretation, but we do not claim that this is a comprehensive typology of the post-replication patterns that may occur. Such patterns may be more complex and idiosyncratic in other topic domains. In order to examine patterns of belief correction/perpetuation, we downloaded citation histories (a list of citing articles) for each original study and classified the valence of a set of pre-replication and post-replication citations as favourable, equivocal, or unfavourable. To examine patterns of citation balance/bias, we manually checked whether post-replication citations of the original study were accompanied by citations to the replication study. Finally, to examine patterns of explicit/absent defence, we extracted and categorised any counter-arguments offered in articles that cited the replication.

# Methods

The study protocol (rationale, methods, and analysis plan) was pre-registered on April 7th 2018 (https://osf.io/eh5qd/). An amended protocol was registered part way through data collection on May 1st 2019, primarily because we extended the sampling frame to cover additional months (https://osf.io/pdvb5/). All deviations from these protocols are explicitly acknowledged in Appendix \@ref(appA). All data exclusions and measures conducted during this study are reported. 

## Design
This was a retrospective observational study consisting of four case studies. Primary outcome variables were annual citation counts for original studies, citation valence (favourable, equivocal, unfavourable), co-citation of original and replication studies, frequency/type of counter-arguments.

## Sample
We examined four case studies in which a prominent pre-registered and multi-laboratory replication study strongly contradicted and outweighed the findings of an original study (Table \@ref(tab:studyDetails)). 

```{r studyDetails}
tibble(
  "Original Study" = c("Baumeister et al. (May, 1998)", "Sripada et al. (April, 2014)", "Strack et al. (May, 1988)", "Caruso et al. (July, 2012)", "Carter et al. (July, 2011)"),
  "Replication study" = c("Hagger et al. (July, 2016)**", "Hagger et al. (July, 2016)**", "Wagenmakers et al. (October, 2016)", "Klein et al. (January, 2014)", "Klein et al. (January, 2014)"),
  "Effect" = c("Ego-depletion", "Ego-depletion", "Facial feedback", "Money priming", "Flag priming"),
  "Total citations to original*" = c("1974", "36", "708", "57", "54"),
  "Original sample size" = c("k = 1 N = 67", "k = 1 N = 26", "k = 1 N = 92", "k = 1 N = 30", "k = 1 N = 70"),
  "Replication sample size" = c("k = 23 N = 2,141", "k = 23 N = 2,141", "k = 17 N = 2,124", "k = 36 N = 6,333", "k = 36 N = 4,896"),
  "Original effect size [95% CI]" = c("d = 2.05 [1.31, 2.79]", "d = 0.68 [0.09, 1.27]", "MD = 0.82 [-0.05, 1.69]", "d = 0.8 [0.05. 1.54]", "d = 0.50 [.01, .99]"),
  "Replication effect size [95% CI]" = c("d = 0.04 [-0.07, 0.15]", "d = 0.04 [-0.07, 0.15]", "MD = 0.03 [-0.11, 0.16]", "d = .01 [-.06, 0.09]", "d = .01 [-.07, 0.08]")
) %>%
  kable(booktabs = T, longtable = T, caption = "Sample sizes and effect sizes for replication studies and original studies. d = Cohen’s d; MD = mean difference; k = number of data collection sites; N = total number of participants; CI = confidence interval. Publication dates are earliest available (i.e., ‘online first’ where relevant).") %>%
  kable_styling(font_size = 10) %>%
  column_spec(1, "1.8cm") %>%
  column_spec(2, "1.8cm") %>%
  column_spec(3, "1.5cm") %>%
  column_spec(4, "1.5cm") %>%
  column_spec(5, "1.5cm") %>%
  column_spec(6, "1.5cm") %>%
  column_spec(7, "1.8cm") %>%
  column_spec(8, "1.8cm") %>%
  footnote(
    symbol = c("Total citations to the original study between the publication date and 31st December, 2019.", "For methodological reasons (see Hagger et al., 2016), the ego-depletion replication was aimed at a classic study in the field (Baumeister et al., 1998), but actually employed a modified computer-based version of the original paradigm (Sripada et al., 2014). We examined post-replication citation patterns for both studies."), symbol_manual = c("*", "**"),
    footnote_as_chunk = T, threeparttable = T
  )
```

## Procedure

### Annual citation counts
Citation histories (i.e., bibliographic records for all articles that cite the original study) from the publication date of each original study through to 31st December, 2019, were downloaded from Clarivate Analytics Web of Science Core Collection accessed via the Charité – Universitätsmedizin Berlin on 12th August, 2020. We also obtained citation histories for a reference class - all articles published in the same journal and the same year as each original study - from the same source. For example, for Baumeister et al. (1998) the reference class was all articles published in 1998 in the Journal of Personality and Social Psychology. Citation counts were standardized in each case study by setting the citation count in the replication year to the standardized value of ‘100’ and then adjusting the counts in other years according to the same transformation ratio. For example, if the raw citation count in the replication year was 1000, citation counts in each year were standardized by dividing by 10. This computation was performed separately for the reference class and citations to the original article.

### Qualitative assessment
Qualitative assessment of citation patterns was limited to a time period starting one year prior to the year of publication of the replication study up until 31st December, 2019, excluding the year in which the replication was published. We excluded the replication year because it may be unreasonable to expect citing articles already in the publication pipeline to cite the replication study. For the Baumeister case, the qualitative analysis was based on a random sample of 40% of citing articles from the pre-replication period and post-replication period due to the large number of citations to the original study (n = `r d_citations %>% filter(case=='baumeister') %>% nrow()`; see Appendix \@ref(appB) for details).

For each citing article undergoing qualitative assessment, we attempted to retrieve the full text via at least two of the institutional libraries we are affiliated with. Inaccessible articles were excluded. For articles for which we could obtain the full text, we classified the research design according to the categories in Table \@ref(tab:articleTypes) and recorded whether the replication study was cited after manual inspection of the reference section (see Table \@ref(tab:patterns): citation balance/bias). 

```{r}
# identify how many times valence classifications were changed following discussion between primary and secondary coder
classChange <- d_contentAnalysis %>%
  filter(excluded == F) %>%
  mutate(classificationChange = citationClassificationOriginal != citationClassificationAgreed) %>%
  count(classificationChange) %>%
  mutate(percent = round(n / sum(n) * 100))
```

To examine the belief correction/perpetuation pattern (Table \@ref(tab:patterns)), the primary coder manually extracted the ‘citation context’ of the original study and the replication study (i.e., all relevant verbatim text surrounding each in-text citation). The primary coder then classified the citation valence as ‘favourable’, ‘equivocal’, ‘unfavourable’, or ‘unclassifiable’. Favourable citations were those used to support a positive claim about the phenomenon of interest whereas unfavourable citations were used to support a negative claim about the phenomenon of interest. Citations were considered equivocal if the authors did not take a predominantly favourable or unfavourable position. Citations that did not endorse or oppose the phenomenon of interest (for example, simply referring to the procedures of the original study) were designated as ‘unclassifiable’. Because this process was inherently subjective, the citation contexts and classifications were also examined by a secondary coder. Disagreements were resolved through discussion and a third coder arbitrated when necessary. Valence classifications by the primary coder were modified after discussion with the secondary coder in `r classChange %>% filter(classificationChange == T) %>% pull(n)` (`r classChange %>% filter(classificationChange == T) %>% pull(percent)`%) cases.

To examine the explicit/absent defence pattern (Table \@ref(tab:patterns)), the primary coder flagged articles that co-cited the original and replication studies that also contained any explicit defence of the original study. Subsequently, two team members (ORA and SC) re-examined all of the flagged cases, extracted verbatim counter-arguments, and developed a post-hoc categorisation scheme that summarised them as concisely and informatively as possible (Table \@ref(tab:caCategories)). Coding disagreements were resolved through discussion and a third coder (TEH) arbitrated when necessary.

In additional exploratory (not pre-registered) analyses, we examined overlap of authorship for articles that provided counter-arguments with (1) any of the authors of the original studies; and (2) any prior collaborators of the first authors of the original studies. These analyses are complicated by the fact that author names in bibliographic records do not always adhere to the same grammatical standards - for example, whether forenames are initialised or middle names are included - so it is not straightforward to isolate individual authors within bibliographic databases. In order to identify prior collaborators of the first authors of the original studies, we downloaded bibliographic records (on 2nd February, 2021) for all papers published by each of the original study first authors, according to their author record in the Web of Science Core Collection. These author records are automatically generated by an algorithm that attempts to identify all documents likely published by an individual author using several variations of their name (for example, “Hardwicke, Tom E.”, “Hardwicke, Tom”,  “Hardwicke, T. E.”), but errors can still occur and incomplete database coverage means that this method likely misses some of the authors’ prior publications and thus some of their collaborators. Nevertheless, the method supports a reasonable lower-bound estimate of authorship overlap with articles providing counter-arguments. To identify authorship overlap, we used string manipulation tools in R to extract only author surnames from bibliographic records and then used string matching to automatically detect the presence of original author or collaborator surnames amongst the surnames of authors of articles that provided counter-arguments. When a match was detected, it was verified by manual examination of the authors’ full names.

# Results

```{r}
excluded_n <- d_contentAnalysis %>%
  filter(excluded == T) %>%
  nrow()
excluded_reason <- d_contentAnalysis %>%
  filter(excluded == T) %>%
  count(exclusionReason)
noAccess_n <- excluded_reason %>%
  filter(exclusionReason == "No access") %>%
  pull(n)
nonEnglish_n <- excluded_reason %>%
  filter(exclusionReason == "Non-English language") %>%
  pull(n)
refCiteOnly_n <- excluded_reason %>%
  filter(exclusionReason == "Cited in references but not in text") %>%
  pull(n)
noCite_n <- excluded_reason %>%
  filter(exclusionReason == "Does not cite target or replication") %>%
  pull(n)
notexcluded_n <- d_contentAnalysis %>%
  filter(excluded == F) %>%
  nrow()
```

In total, `r d_citations %>% nrow()` articles cited one of the original studies of which `r d_contentAnalysis %>% nrow()` articles (after taking a 40% random sample in the Baumeister case) fell within the time period designated for qualitative assessment. Of these `r d_contentAnalysis %>% nrow()` articles, we excluded `r excluded_n` from the qualitative analysis because (1) we could not access the full text (n = `r noAccess_n`); (2) they were non-English language (n = `r nonEnglish_n`); (3) they included a citation to the original study in the reference section, but not in the main text (n = `r refCiteOnly_n`); or (4) manual inspection indicated that they did not actually appear to cite the original study at all (n = `r noCite_n`). Research design classifications for the remaining `r notexcluded_n` articles included in the qualitative analysis are shown in Table \@ref(tab:articleTypes).

## Article characteristics

```{r articleTypes}
d_contentAnalysis %>%
  filter(excluded == F) %>%
  count(articleType) %>%
  mutate("Count (%)" = paste0(n, " (", round(n / sum(n), 2) * 100, ")")) %>%
  select("Article type" = articleType, "Count (%)", -n) %>%
  apa_table(caption = "Counts and percentages for research design classifications of articles included in qualitative analyses")
```

## Annual citation counts and citation valence

Figure \@ref(fig:citeCurves) shows standardized annual citation counts for each original study and the respective reference class (citations to all articles published in the same year and same journal as the original study), and classifications of citation valence (favourable, equivocal, unfavourable, unclassifiable or excluded). The data can also be viewed in tabular format in Appendix \@ref(tab:tabularData). All counts (n) reported in the text and table are raw counts (i.e., not standardized).

```{r citeCurves, fig.cap='Standardized annual citation counts (solid line) for the five original studies with citation valence (favourable, equivocal, unfavourable, unclassifiable) illustrated by coloured areas in pre-replication and post-replication assessment periods. Dashed line depicts citations to the reference class (all articles published in the same journal and same year as the target article). Annual citation counts are standardized against the year in which the replication was published (citation counts in the replication year, indicated by a black arrow, are set at the standardized value of 100). Citation valence classifications for the Baumeister case are extrapolated to all articles in the assessment period based on a 40% random sample.', fig.width=12, fig.height=12.5, fig.path='figs/', dev=c('png', 'pdf')}
ggarrange(
  ggarrange(citationCurve("strack", thisTitle = "Strack et al. (1988)", standardized = T, plotReference = T, areaPlot = "classification"),
    citationCurve("baumeister", thisTitle = "Baumeister et al. (1998)", standardized = T, plotReference = T, areaPlot = "classification"),
    nrow = 2, ncol = 1, common.legend = T
  ),
  ggarrange(citationCurve("sripada", thisTitle = "Sripada et al. (2014)", standardized = T, plotReference = T, areaPlot = "classification"),
    citationCurve("carter", thisTitle = "Carter et al. (2011)", standardized = T, plotReference = T, areaPlot = "classification"),
    citationCurve("caruso", thisTitle = "Caruso et al. (2013)", standardized = T, plotReference = T, areaPlot = "classification"),
    nrow = 1, ncol = 3, common.legend = F
  ),
  nrow = 2, ncol = 1, heights = c(2, 1)
) %>%
  annotate_figure(
    left = text_grob("Standardized citation count", rot = 90, size = 20),
    bottom = text_grob("Publication year", size = 20)
  )
```

```{r}
# compute maximum difference for 'other' cases (Sripada, Carter, Caruso)
maxDiff <- max(c(d_summary %>% filter(case == "carter" & pubYear == "2019") %>% pull(citesTarget) - d_summary %>% filter(case == "carter" & pubYear == "2012") %>% pull(citesTarget), d_summary %>% filter(case == "caruso" & pubYear == "2019") %>% pull(citesTarget) - d_summary %>% filter(case == "caruso" & pubYear == "2013") %>% pull(citesTarget), d_summary %>% filter(case == "sripada" & pubYear == "2019") %>% pull(citesTarget) - d_summary %>% filter(case == "sripada" & pubYear == "2015") %>% pull(citesTarget)))
```

After the replication was published, citations to the reference classes were continuing their trend to plateau (Baumeister case) or increase (other cases). By contrast, citations to the original study appeared to undergo a modest decline in the Strack case (decreasing from `r d_summary %>% filter(case == 'strack', pubYear == '2015') %>% pull(citesTarget)` to `r d_summary %>% filter(case == 'strack', pubYear == '2019') %>% pull(citesTarget)` between 2015 and 2019), and a small decline followed by a small increase in the Baumeister case (increasing from `r d_summary %>% filter(case == 'baumeister', pubYear == '2015') %>% pull(citesTarget)` to `r d_summary %>% filter(case == 'baumeister', pubYear == '2019') %>% pull(citesTarget)` between 2015 and 2019). In the other cases (Sripada, Carter, Caruso), the total citation counts were much lower and there was considerable variability in the post-replication citation patterns; nevertheless, there was no substantial change in annual citations from pre- to post- replication in these three cases (the maximum difference was +`r maxDiff` citations).

```{r}
preRepFavRange <- d_summary %>%
  filter(case == "carter" & pubYear == "2013" | case == "caruso" & pubYear == "2013" | case %in% c("sripada", "baumeister", "strack") & pubYear == "2015") %>%
  pull(favourable_prop) %>%
  range()

preRepUnfav_Strack <- d_summary %>%
  filter(case == "strack", pubYear == "2015") %>%
  pull(unfavourable_prop)
postRepUnfav_Strack <- d_summary %>%
  filter(case == "strack", pubYear == "post-replication") %>%
  pull(unfavourable_prop)
preRepFav_Strack <- d_summary %>%
  filter(case == "strack", pubYear == "2015") %>%
  pull(favourable_prop)
postRepFav_Strack <- d_summary %>%
  filter(case == "strack", pubYear == "post-replication") %>%
  pull(favourable_prop)

preRepFav_Bau <- d_summary %>%
  filter(case == "baumeister", pubYear == "2015") %>%
  pull(favourable_prop)
postRepFav_Bau <- d_summary %>%
  filter(case == "baumeister", pubYear == "post-replication") %>%
  pull(favourable_prop)

unfavN_Bau2017 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2017") %>%
  pull(unfavourable)
unfavProp_Bau2017 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2017") %>%
  pull(unfavourable_prop)
unfavN_Bau2018 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2018") %>%
  pull(unfavourable)
unfavProp_Bau2018 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2018") %>%
  pull(unfavourable_prop)
unfavN_Bau2019 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2019") %>%
  pull(unfavourable)
unfavProp_Bau2019 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2019") %>%
  pull(unfavourable_prop)
```
Prior to the replication, the vast majority of citations were favourable for all five articles (range `r round(preRepFavRange[1]*100,0)`% to `r round(preRepFavRange[2]*100,0)`%). In most cases (Strack, Sripada, Carter, and Caruso) there was a small post-replication increase in unfavourable citations and a small decrease in favourable citations, indicating a modest active correction pattern. However, the overall number of unfavourable citations was very low and there was still a substantial majority of favourable citations. For example, in the Strack case, unfavourable citations increased from `r preRepUnfav_Strack`% in the pre-replication period (2015) to `r round(postRepUnfav_Strack,2)*100`% in the post-replication period, whilst favourable citations decreased from `r round(preRepFav_Strack,2)*100`% to `r round(postRepFav_Strack,2)*100`%. In the Baumeister case, the proportion of favourable citations remained stable from pre-replication (`r round(preRepFav_Bau,2)*100`%) to post-replication (`r round(postRepFav_Bau,2)*100`%), a pattern consistent with belief perpetuation. The very small number of unfavourable citations (2017: n = `r unfavN_Bau2017`, `r round(unfavProp_Bau2017,2)*100`%; 2018: n = `r unfavN_Bau2018`, `r round(unfavProp_Bau2018,2)*100`%; 2019: n = `r unfavN_Bau2019`, `r round(unfavProp_Bau2019,2)*100`%) suggests that this is largely an unchallenged belief perpetuation pattern (see Table \@ref(tab:patterns)).

## Citation balance and citation bias

```{r}
citesRep_Bau2017 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2017") %>%
  pull(citesRep_yes_prop)
citesRep_Bau2019 <- d_summary %>%
  filter(case == "baumeister", pubYear == "2019") %>%
  pull(citesRep_yes_prop)

citesRep_Strack2017 <- d_summary %>%
  filter(case == "strack", pubYear == "2017") %>%
  pull(citesRep_yes_prop)
citesRep_Strack2019 <- d_summary %>%
  filter(case == "strack", pubYear == "2019") %>%
  pull(citesRep_yes_prop)

citesRep_carter_caruso_max <- d_summary %>%
  filter(case %in% c("carter", "caruso")) %>%
  pull(citesRep_yes_prop) %>%
  max(na.rm = T)

citesRep_sripada <- d_summary %>%
  filter(case == "sripada") %>%
  pull(citesRep_yes_prop) %>%
  min(na.rm = T)
```

Figure \@ref(fig:repPlot) shows the proportion of citing articles that also cited or did not cite the replication study after it was published (excluding the publication year itself). The data can also be viewed in tabular format in Appendix \@ref(tab:tabularData). In the Strack and Baumeister cases, a considerable majority of articles citing the original study did not cite the replication study, indicating substantial citation bias. In the Baumeister case the proportion of articles citing the replication study remained stable (`r round(citesRep_Bau2017,2)*100`% in 2017, `r round(citesRep_Bau2019,2)*100`% in 2019). In the Strack case, the proportion increased from `r round(citesRep_Strack2017,2)*100`% to `r round(citesRep_Strack2019,2)*100`%. In the Carter and Caruso cases, the proportion never exceeded `r round(citesRep_carter_caruso_max,2)*100`%, also consistent with substantial citation bias. In the Sripada case, it was much more common for the replication study to be cited (>`r round(citesRep_sripada,2)*100`%) reflecting a balanced citation pattern.

```{r repPlot, fig.cap='Standardized annual citation counts (solid line) for the five original studies with citation balance/bias (i.e., whether the replication is cited) illustrated by coloured areas in the post-replication assessment period. Dashed line depicts citations to the reference class (all articles published in the same journal and same year as the target article). Annual citation counts are standardized against the year in which the replication was published (citation counts in the replication year, indicated by a black arrow, are set at the standardized value of 100). Replication citation proportions for the Baumeister case are extrapolated to all articles in the assessment period based on a 40% random sample.', fig.width=12, fig.height=12.5, fig.path='figs/', dev=c('png', 'pdf')}
ggarrange(
  ggarrange(citationCurve("strack", thisTitle = "Strack et al. (1988)", areaPlot = "citesReplication", zoom = T),
    citationCurve("baumeister", thisTitle = "Baumeister et al. (1998)", areaPlot = "citesReplication", zoom = T),
    nrow = 2, ncol = 1, common.legend = T
  ),
  ggarrange(citationCurve("sripada", thisTitle = "Sripada et al. (2014)", areaPlot = "citesReplication", zoom = T),
    citationCurve("carter", thisTitle = "Carter et al. (2011)", areaPlot = "citesReplication", zoom = T),
    citationCurve("caruso", thisTitle = "Caruso et al. (2013)", areaPlot = "citesReplication", zoom = T),
    nrow = 1, ncol = 3, common.legend = F
  ),
  nrow = 2, ncol = 1, heights = c(2, 1)
) %>%
  annotate_figure(
    left = text_grob("Standardized citation count", rot = 90, size = 20),
    bottom = text_grob("Publication year", size = 20)
  )
```

## Explicit defence and absent defence

```{r}
# prepare table of counter-argument frequency

## get proportion of counter arguments for each individual case
ca <- d_contentAnalysis %>%
  filter(citesReplication == T) %>%
  group_by(case, counterArguments, .drop = F) %>%
  summarise(n = n()) %>%
  mutate(all = paste0(n, " (", round(n / sum(n), 2), ")")) %>%
  select(-n)

## get proportion of counter arguments across all cases
ca_everycase <- d_contentAnalysis %>%
  filter(citesReplication == T) %>%
  group_by(counterArguments, .drop = F) %>%
  summarise(n = n()) %>%
  mutate(
    case = factor("all cases"),
    all = paste0(n, " (", round(n / sum(n), 2), ")")
  ) %>%
  select(-n)

ca <- rbind(ca, ca_everycase)

## get proportion of counter arguments for each individual case where articles cited original favourably
ca_fav <- d_contentAnalysis %>%
  filter(citesReplication == T) %>%
  filter(citationClassificationAgreed == "favourable") %>%
  group_by(case, counterArguments, .drop = F) %>%
  summarise(n = n()) %>%
  mutate(fav = paste0(n, " (", round(n / sum(n), 2), ")")) %>%
  select(-n)

## get proportion of counter arguments across all cases where articles cited original favourably
ca_fav_everycase <- d_contentAnalysis %>%
  filter(citesReplication == T) %>%
  filter(citationClassificationAgreed == "favourable") %>%
  group_by(counterArguments, .drop = F) %>%
  summarise(n = n()) %>%
  mutate(
    case = factor("all cases"),
    fav = paste0(n, " (", round(n / sum(n), 2), ")")
  ) %>%
  select(-n)

ca_fav <- rbind(ca_fav, ca_fav_everycase)

# for caFreq table (below)
caFreq <- left_join(ca, ca_fav) %>%
  pivot_wider(id_cols = case, names_from = counterArguments, values_from = c(all, fav)) %>%
  rename("No" = all_FALSE, "No " = fav_FALSE, "Yes" = all_TRUE, "Yes " = fav_TRUE)
```

```{r }
# for counter-argument categories table (below)
counterArgCategories <- d_contentAnalysis %>%
  filter(citesReplication == T, counterArguments == T) %>%
  summarise(evidence = sum(as.logical(evidenceCounter), na.rm = T), methods = sum(as.logical(methodsCounter), na.rm = T), expertise = sum(as.logical(expertiseCounter), na.rm = T))
```

Table \@ref(tab:caFreq) shows whether articles that cited the original study and replication study (“co-citing articles”), and the subset of co-citing articles that cited the original study favourably, provided any explicit counter-arguments to defend the credibility of the original finding (an explicit defence) or not (an absent defence). Overall, fewer than half of the `r d_contentAnalysis %>% filter(citesReplication==T) %>% nrow()` co-citing articles provided any counter-arguments. Of the `r d_contentAnalysis %>% filter(citesReplication==T,citationClassificationAgreed=='favourable') %>% nrow()` co-citing articles that cited the original study favourably, around half provided counter-arguments. We identified `r counterArgCategories %>% mutate(totalCA = sum(evidence,methods,expertise)) %>% pull(totalCA)` discrete counter-arguments in `r d_contentAnalysis %>% filter(citesReplication==T,counterArguments==T) %>% nrow()` citing articles (`r d_contentAnalysis %>% filter(citesReplication==T,counterArguments==T) %>% distinct(doi) %>% nrow()` of which were unique articles, as `r d_contentAnalysis %>% filter(citesReplication==T,counterArguments==T) %>% count(doi) %>% filter(n>1) %>% nrow()` of them were cited in two of the case studies) and allocated them to one of three categories (Table \@ref(tab:caCategories)).

```{r caFreq}
caFreq %>%
  kable("latex", booktabs = T, caption = "Counts and percentages (in brackets) for whether articles that cited both the original study and replication study provided any explicit argumentation to defend the original study. Data are displayed for co-citing articles with any citation valence classification and the subset of co-citing articles with favourable citation valence classifications.") %>%
  add_header_above(c(" " = 1, "All citation valences" = 2, "Favourable citation valences" = 2))
```

```{r caCategories}
counterArgCategories %>%
  apa_table(caption = "Categorisation of counter-arguments provided to defend the original study in light of the contradictory replication result. 57 discrete counter-arguments were identified in 50 articles (45 unique articles across cases).")
```


```{r}
# exploratory analyses of articles containing counter-arguments

# get all articles containing counter arguments
d_args <- d_contentAnalysis %>%
  filter(counterArguments == T)

# which journals are counter-argument articles published in?
ca_journals <- left_join(d_args, d_citations, by = "UT") %>%
  distinct(doi, .keep_all = T) %>% # unique articles only
  count(SO) %>%
  arrange(desc(n)) %>%
  rename("journal" = SO)
```

```{r}
# get article types for counter-argument articles
ca_artTypes <- d_args %>%
  distinct(doi, .keep_all = T) %>% # unique articles only
  count(articleType)

# extract information for in text reporting
ca_noData <- ca_artTypes %>%
  filter(articleType %in% c("No empirical data")) %>%
  pull(n)
ca_novData <- ca_artTypes %>%
  filter(articleType %in% c("Empirical data - field study", "Empirical data - laboratory study", "Empirical data - multiple study types are reported", "Empirical data - survey")) %>%
  pull(n) %>%
  sum()
ca_existData <- ca_artTypes %>%
  filter(articleType %in% c("Data synthesis - meta-analysis", "Empirical data - commentary including analysis")) %>%
  pull(n) %>%
  sum()
```

```{r} 
# now look at authorship overlap - a problem here is identifying unique individuals due to non-standard meta-data for author names. To address this we will convert full names to surnames then manually verify any automatically identified matches.

# get original author surnames for all cases
org_auths_df <- data.frame(case = caseNames, authNames = c(
  "Baumeister, Bratslavsky, Muraven, Tice", # Baumeister, RF; Bratslavsky, E; Muraven, M; Tice, DM,
  "Sripada, Kessler, Jonides", # Sripada, C; Kessler, D; Jonides, J,
  "Strack, Martin, Stepper", # STRACK, F; MARTIN, LL; STEPPER, S,
  "Carter, Ferguson, Hassin", # Carter, TJ; Ferguson, MJ; Hassin, RR,
  "Caruso, Vohs, Baxter, Waytz"
)) # Caruso, EM; Vohs, KD; Baxter, B; Waytz, A)

# convert to a list of original author surnames
org_auths_list <- str_c(org_auths_df$authNames, collapse = ", ") %>%
  str_replace_all(",", "") %>%
  str_split(" ") %>%
  unlist()

# author surnames for counter-argument papers
ca_auth_list <- d_args %>%
  distinct(doi, .keep_all = T) %>%
  pull(authors) %>%
  str_c(collapse = "; ")

# frequency of authors in counter-arg papers
ca_auth_freq <- ca_auth_list %>%
  str_split(";") %>%
  table(dnn = "name") %>%
  as.data.frame() %>%
  arrange(desc(Freq)) %>%
  rowwise() %>%
  mutate(surname = str_split(name, ",")[[1]][1]) # extract surnames

# frequency of original authors also authoring counter-arg papers
ca_auth_surnames <- ca_auth_freq$surname %>% str_c(collapse = " ")
org_auths_ca <- org_auths_list[str_detect(ca_auth_surnames, org_auths_list)]

# now extract surnames of co-authors of original first authors
d_coauthors <- d_coauthors %>%
  rowwise() %>%
  mutate(surnames = str_split(Authors, "; ")[[1]] %>% str_extract("[^,]+") %>% str_c(sep = "", collapse = ";"))

# and convert to a list
co_auths_list <- d_coauthors$surnames %>%
  str_to_lower() %>%
  str_c(";", collapse = "") %>%
  str_split(";") %>%
  as.data.frame(col.names = "coauthors") %>%
  filter(coauthors != "baumeister") %>%
  distinct(coauthors) %>%
  count(coauthors) %>%
  filter(coauthors != "") %>%
  pull(coauthors)

# now we will do some string matching to identify overlap of authorship between counter-argument article (CAA) authors and (1) original authors and (2) collaborators of original first authors.

d_args <- d_args %>%
  distinct(doi, .keep_all = T) %>%
  rowwise() %>%
  mutate(
    surnames = str_split(authors, "; ")[[1]] %>% str_extract("[^,]+") %>% str_c(sep = "", collapse = " "), # extract CAA author surnames
    org_auth_detected = ifelse(any(str_detect(str_to_lower(surnames), str_to_lower(paste0("\\b", org_auths_list, "\\b")))), T, F), # check if each CAA author surname is amongst original author surnames
    coauth_detected = ifelse(any(str_detect(str_to_lower(surnames), str_to_lower(paste0("\\b", co_auths_list, "\\b")))), T, F)
  ) # check if each CAA author surname is amongst original first author collaborator surnames


# after automated matching, we did a manual check to verify
# because the automated matching was based on surnames, sometimes the wrong individual is flagged as a match. Manual checking highlights these cases. If necessary, we override the match classification of the automated matching.

d_args <- d_args %>%
  mutate(coauth_detected = case_when(
    authors == "Daley, Ken; Howell, Robert" ~ F,
    authors == "Englert, Chris; Zavery, Alafia; Bertrams, Alex" ~ T, # 2 detected
    authors == "Wang, Yan; Wang, Guosen; Chen, Qiuju; Li, Lin" ~ F,
    authors == "Kuehl, Tim; Bertrams, Alex" ~ T,
    authors == "Friese, Malte; Loschelder, David D.; Gieseler, Karolin; Frankenbach, Julius; Inzlicht, Michael" ~ T,
    authors == "Staller, Mario S.; Mueller, Marcel; Christiansen, Paul; Zaiser, Benjamin; Koerner, Swen; Cole, Jon C." ~ F,
    authors == "Achtziger, Anja; Alos-Ferrer, Carlos; Wagner, Alexander K." ~ T,
    authors == "Dang, Junhua; Liu, Ying; Liu, Xiaoping; Mao, Lihua" ~ F,
    authors == "Wolff, W; Baumann, L; Englert, C" ~ T,
    authors == "Alos-Ferrer, Carlos; Ritschel, Alexander; Garcia-Segarra, Jaume; Achtziger, Anja" ~ T,
    authors == "Stroebe, Wolfgang" ~ T,
    authors == "Schwarz, Norbert; Lee, Spike W. S." ~ T,
    authors == "Nelson, Leif D.; Simmons, Joseph; Simonsohn, Uri" ~ F,
    authors == "Strack, Fritz" ~ F, # this is an original author
    authors == "Cheng, Yongtian; Li, Johnson Ching-Hong; Liu, Xiyao" ~ F,
    authors == "Stroebe, Wolfgang" ~ T,
    authors == "Caruso, Eugene M.; Shapira, Oren; Landy, Justin F." ~ F, # this is an original author
    authors == "Vohs, Kathleen D." ~ F, # this is an original author
    T ~ F # everything else is false
  ))

# extract information for in text reporting
co_auths_overlap <- d_args %>%
  count(coauth_detected) %>%
  filter(coauth_detected == T) %>%
  pull(n)

org_auths_overlap <- d_args %>%
  count(org_auth_detected) %>%
  filter(org_auth_detected == T) %>%
  pull(n)

overlapArtTypes <- d_args %>%
  filter(coauth_detected == T | org_auth_detected == T) %>%
  count(articleType)
```

In additional exploratory analyses (not pre-registered) we examined other characteristics of the `r d_contentAnalysis %>% filter(citesReplication==T,counterArguments==T) %>% distinct(doi) %>% nrow()` unique articles that contained counter-arguments. The articles were published in `r nrow(ca_journals)` individual journals, with *Frontiers in Psychology* publishing `r ca_journals %>% filter(journal == 'FRONTIERS IN PSYCHOLOGY') %>% pull(n)` of the articles, *Social Psychology* publishing `r ca_journals %>% filter(journal == 'SOCIAL PSYCHOLOGY') %>% pull(n)` of the articles, and all other journals publishing only 1 or 2 of the articles. `r ca_noData` of the articles did not involve empirical data, `r ca_existData` involved reanalysis or meta-analysis of existing data, and `r ca_novData` involved collection of novel data. The articles had `r nrow(ca_auth_freq)` individual authors of whom all contributed to a single article except for `r ca_auth_freq %>% filter(Freq > 1) %>% nrow()` individuals who had (co)authored 2 articles. `r org_auths_overlap` articles were (co)authored by one of the original authors and `r co_auths_overlap` articles were (co)authored by at least one prior collaborator of one of the first authors of the original articles. `r overlapArtTypes %>% filter(articleType == "No empirical data") %>% pull(n)` of these articles did not involve empirical data and `r overlapArtTypes %>% filter(articleType != "No empirical data") %>% pull(n) %>% sum()` of them involved novel data collection.


Discussion
It has been proposed that replication studies can facilitate scientific self-correction by modifying scientists’ belief in the credibility of published findings (Ioannidis, 2012; Vazire & Holcombe, 2020; Zwaan et al., 2018); however, the extent to which this occurs in practice is unclear. In this study, we investigated how the research community responded to four strongly contradictory replications in the field of psychology by examining post-replication citation patterns for original studies. We observed some ‘progressive’ response in the form of modest active correction patterns (a small decline in favourable citations and a small increase in unfavourable citations), but also even more prominent ‘regressive’ responses in the form of unchallenged belief perpetuation (sustained levels of favourable citations and few unfavourable citations, particularly in the Baumeister case) and considerable citation bias (neglecting to cite the replication study; in all cases aside from Sripada). When authors cited the original study favourably despite the replication result, only half of the articles provided any explicit counter arguments in defence of the original study (an explicit defence). Overall, these findings are consistent with prior observations that favourable citation patterns can continue relatively unperturbed by subsequent publication of contradictory results in studies with lower risk of bias (Tatsioni et al., 2007) or even by full retraction (Budd et al., 1998), and that positive (supportive) evidence is preferentially cited relative to negative (non-supportive) evidence once a theory gets entrenched despite overwhelming evidence against it (Greenberg, 2009).

It is reasonable to question whether the replication results in these case studies should (rationally) have instigated belief change in the research community and thus triggered a more sizable decline in favourable citations than we observed. It is important to note that the case studies we selected were deliberately chosen because the replication results were superior to the original results in terms of both credibility and evidential value. The replications were pre-registered, muli-laboratory studies with large sample sizes. By contrast, the original studies had much smaller sample sizes, and arguably had a much higher risk of bias, as they were not pre-registered, were performed by single teams, and arose in domains affected by publication bias and other questionable research practices (E. C. Carter et al., 2015; Coles et al., 2019; Vadillo, 2019; Vadillo et al., 2016). Thus, it seems reasonable to suggest that the compelling replication results should have reduced belief in the credibility of the original results.

The clear evidence of citation bias that our study documents may have two main contributory factors: (1) a lack of awareness about the replication results; and/or (2) a decision to ignore the replication results. The practical issue of awareness is not necessarily straightforward to address. Individual scientists can find it difficult to keep up-to-date with the voluminous literature that is relevant to their research. Recently, the reference manager Zotero has introduced a feature that alerts users when an article in their database has been retracted (Zotero, 2019). One could imagine a similar feature being introduced for replication studies, perhaps based on databases that explicitly identify replication studies such as Curate Science (https://curatescience.org/). However, it is much less straightforward to define a relevant replication study (Neuliep & Crandall, 1993) and users would need to be alerted that this requires some scientific judgement rather than simple article meta-data. Another solution could be to encourage researchers to focus less on individual studies and more on up-to-date evidence summaries (i.e., reviews and meta-analyses) in which relevant evidence is systematically identified and collated. This would require that high-quality and contemporary evidence summaries are available; however, in psychology systematic reviews and meta-analyses can be of low quality and their results may still be inflated and irreproducible (Kvarven et al., 2020; Maassen et al., 2020; Polanin et al., 2020). Moreover, most empirical studies are not included in any form of evidence synthesis (Hardwicke et al., 2020).

The second issue of authors ignoring highly relevant replications is undesirable and implies a biased appraisal of the evidence. Contradictory replication results do not necessarily nullify the credibility of an original study (Collins, 1985; Earp & Trafimow, 2015; Maxwell et al., 2015), but we would still expect highly relevant replication results to be cited and explicitly debated. In fact, we found that when authors continued to favourably cite original studies despite the replication results, around half did not provide explicit counter-arguments in defence of the original study. Although our study did not examine researchers’ individual beliefs, one recent study found that when research psychologists were confronted with replication evidence, they often did update their (self-reported) beliefs, albeit modestly (McDiarmid et al., 2021). However, there are several reasons to be uncertain about whether the results from this artificial setting might generalize to real-world settings, including potential observer bias (i.e., participants behaving differently because they are under observation and/or responding to the perceived expectations of the research team) and the possibility that individuals may behave differently in settings where they have substantial personal investment and may be publicly scrutinized. Additionally, cognitive psychology studies have obtained some evidence of a ‘continued influence effect’ wherein an individual's beliefs and behaviour can continue to be influenced by false or misleading information despite subsequent efforts to reject it (Lewandowsky et al., 2012). It is plausible that various cognitive biases, such as confirmation bias (preferentially seeking out and processing evidence that supports pre-existing beliefs) or motivated reasoning (constructing and evaluating arguments based on what is desirable rather than what is rationally justifiable), may partly explain researchers' tendency to ignore or dismiss the replication evidence (Bishop, 2019; Kunda, 1990; Nickerson, 1998). 

We observed that when counter-arguments were raised, most of them tried to dismiss the contradictory replication by claiming that the original and the replication studies differed in important ways that moderated the absence/presence of the effect under investigation. In some cases, authors pointed to evidence from other studies as a rationale for their continued belief in the effect. In a minority of cases, the authors challenged the competence of replicators. We found that articles presenting counter-arguments were published in a variety of journals (rather than clustered in a few journals), and involved collection of new data in around half of the cases. They were also published by a sizeable group of investigators, only a minority of whom were one of the original authors or had previously collaborated with one of the original first authors. This suggests that the explicit defence of the original study came from a variety of sources rather than being confined to a small number of investigators. However, note that this analysis may underestimate authorship overlap due to difficulties isolating individual researcher identities (see methods section) and articles published in the same year as the replication were not included.

The findings presented here are inherently limited by the observational nature of the study design, which complicates straightforward conclusions about the causal impact of the replications. Although the use of a reference class enables us to detect the influence of exogenous factors to some extent, we cannot rule out their contribution. For example, the modest decline in favourable citations observed in most cases could be attributable, at least in part, to a more general awareness in the research community about methodological issues (for example, that the sample sizes of the original studies may not have provided sufficient statistical power). We have also focused only on the replication study and the original study in each case study, without considering the impact of other potentially relevant events. Notably, meta-research studies have detected signatures of publication bias and other questionable research practices in the fields to which these case studies belong (E. C. Carter et al., 2015; Coles et al., 2019; Vadillo, 2019; Vadillo et al., 2016) and other relevant replication studies contesting prior findings have been published (e.g., Rohrer et al., 2015). 

We have only been able to gauge reactions to replications to the extent that they are reflected in citation patterns. Thus, for example, our method would not detect if there had been a self-correction effect amongst individuals who would not typically cite the original study, such as students, members of the public, or researchers working in other fields. A contested study may continue to be  cited favourably by its proponents who remain working in the field. This will suffice to create belief perpetuation in the published literature, even though other scientists may simply no longer be interested in getting involved with such a strongly contested research topic. Relatedly, we did not examine citation patterns beyond 3-5 years post-replication. Some perspectives envision scientific self-correction unfolding over a much longer time scale (Lauden, 1981; Peterson & Panofsky, 2020) and suggest the process is characterised less by the impact of individual study results and more by the gradual accumulation of converging evidence, gradual revision of theoretical understanding, and/or informal sociological processes (e.g., researchers choosing alternative topics to study). Thus, whilst the current findings may contradict the expectations of a more direct and expedited view of the corrective impact of replication studies (Ioannidis, 2012; Vazire & Holcombe, 2020), they are not necessarily inconsistent with a slower and more indirect process of self-correction.

Generalization beyond these four case studies requires caution. Of note, the replication studies examined here were some of the first large-scale multi-laboratory replication attempts conducted in the field of psychology. This gave them particular prominence and initiated considerable debate, resulting in broader ramifications beyond the research community that typically studies the topics under scrutiny (Nelson et al., 2018). Also of note, we deliberately selected case studies where the replication studies were high-profile and had yielded high credibility evidence that strongly contradicted and outweighed the original findings. A self-correction effect may be less expected in cases where replication results are more ambiguous, less consequential, or less well-known. For example, in a situation where two high credibility studies with similar evidential value yield contradictory results, it would be premature to lose confidence in one of the studies before further investigation has probed the cause of the discrepancy. Pursuit of potential moderating factors may be entirely rational in such circumstances (Gershman, 2019).

We should also highlight that particular aspects of our study were inherently subjective, specifically, the identification of citation context, the classification of citation valence, and the identification, extraction, and categorisation of counter-arguments. In order to minimize subjectivity, a team of six investigators performed all coding in duplicate, with a third investigator arbitrating if necessary. As disagreements between primary and secondary coders were infrequent, we are confident that the classifications are meaningful, but there may be some edge cases when an independent observer might reasonably disagree with our classifications.

In conclusion, post-replication citation patterns in four case studies indicated that the anticipated corrective impact of strongly contradictory replication results did not materialise to any substantive degree. A lack of awareness of replications and/or a decision to discount them appears to have played a significant role. This highlights potential practical problems with the discoverability of replication studies and psychological or sociological issues related to belief change. The findings also indicate that scientific self-correction may not be as expedient or straightforward as one might hope (Ioannidis, 2012), adding further impetus towards efforts to improve the quality of the academic literature (Hardwicke et al., 2020; Nelson et al., 2018).

# Materials, data, and analysis code availability
All data, materials, and analysis scripts related to this study are publicly available on the Open Science Framework (https://osf.io/w8h2q/). To facilitate reproducibility this manuscript was written by interleaving regular prose and analysis code using knitr (Xie, 2017) and papaja (Aust & Barth, 2020), and is available in a Code Ocean container (https://doi.org/10.24433/CO.4225975.v1) which re-creates the software environment in which the original analyses were performed.

# Funding statement
The authors received no specific funding for this work. The Meta-Research Innovation Center at Stanford (METRICS) is supported by a grant from the Laura and John Arnold Foundation. The Meta-Research Innovation Center Berlin (METRIC-B) is supported by a grant from the Einstein Foundation and Stiftung Charité. The work of John Ioannidis is supported by an unrestricted grant from Sue and Bob O’Donnell. Robert T. Thibault is supported by a postdoctoral fellowship from the Fonds de la recherche en santé du Québec. Tom E. Hardwicke receives funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No. 841188. Denes Szucs receives funding from the James S. McDonnell Foundation 21st Century Science Initiative in Understanding Human Cognition (grant number 220020370). Olmo R. van den Akker is supported by a Consolidator Grant (IMPROVE) from the European Research Council (ERC; grant no. 726361).

# Conflict of interest statement
The authors declare no conflicts of interest.

# Author contributions
TEH, DS, and JPAI designed the study. TEH, DS, RTT, SC, ORA, and MN performed the data extraction and coding. TEH and SC performed the data analysis. TEH wrote the manuscript. All authors provided feedback and approved the manuscript.

# References

Aust, F., & Barth, M. (2020). Papaja: Create apa manuscripts with rmarkdown. https://github.com/crsh/papaja

Bastiaansen, J. A., de Vries, Y. A., & Munafò, M. R. (2015). Citation distortions in the literature on the serotonin-transporter-linked polymorphic region and amygdala activation. Biological Psychiatry, 78(8), E35–E36. https://doi.org/10.1016/j.biopsych.2014.12.007

Baumeister, R. E., Bratslavsky, E., Muraven, M., & Tice, D. M. (1998). Ego depletion: Is the active self a limited resource? Journal of Personality and Social Psychology, 74(5), 1252–1265. https://doi.org/10.1037/0022-3514.74.5.1252

Bishop, D. V. (2019). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture: Quarterly Journal of Experimental Psychology, 73(1), 1–19. https://doi.org/10.1177/1747021819886519

Budd, J. M., Sievert, M., & Schultz, T. R. (1998). Phenomena of retraction: Reasons for retraction and citations to the publications. JAMA, 280(3), 296–297. https://doi.org/10.1001/jama.280.3.296

Carter, E. C., Kofler, L. M., Forster, D. E., & McCullough, M. E. (2015). A series of meta-analytic tests of the depletion effect: Self-control does not seem to rely on a limited resource. Journal of Experimental Psychology. General, 144(4), 796–815. https://doi.org/10.1037/xge0000083

Carter, T. J., Ferguson, M. J., & Hassin, R. R. (2011). A single exposure to the american flag shifts support toward republicanism up to 8 months later. Psychological Science, 22(8), 1011–1018. https://doi.org/10.1177/0956797611414726

Caruso, E. M., Vohs, K. D., Baxter, B., & Waytz, A. (2013). Mere exposure to money increases endorsement of free-market systems and social inequality. Journal of Experimental Psychology: General, 142(2), 301–306. https://doi.org/10.1037/a0029288

Coles, N. A., Larsen, J. T., & Lench, H. C. (2019). A meta-analysis of the facial feedback literature: Effects of facial feedback on emotional experience are small and variable. Psychological Bulletin, 145(6), 610–651. https://doi.org/10.1037/bul0000194

Collins, H. M. (1985). Changing order: Replication and induction in scientific practice. Sage Publications.

Earp, B. D., & Trafimow, D. (2015). Replication, falsification, and the crisis of confidence in social psychology. Frontiers in Psychology, 6, 621. https://doi.org/10.3389/fpsyg.2015.00621

Etz, A., & Vandekerckhove, J. (2016). A Bayesian perspective on the Reproducibility Project: Psychology. PLOS ONE, 11(2), e0149794. https://doi.org/10.1371/journal.pone.0149794

Gershman, S. J. (2019). How to never be wrong. Psychonomic Bulletin & Review, 26(1), 13–28. https://doi.org/10.3758/s13423-018-1488-8

Greenberg, S. A. (2009). How citation distortions create unfounded authority: Analysis of a citation network. BMJ, 339. https://doi.org/10.1136/bmj.b2680

Hagger, M. S., Chatzisarantis, N. L. D., Alberts, H., Anggono, C. O., Batailler, C., Birt, A. R., Brand, R., Brandt, M. J., Brewer, G., Bruyneel, S., Calvillo, D. P., 
Campbell, W. K., Cannon, P. R., Carlucci, M., Carruth, N. P., Cheung, T., Crowell, A., De Ridder, D. T. D., Dewitte, S., … Zwienenberg, M. (2016). A multilab preregistered replication of the ego-depletion effect. Perspectives on Psychological Science, 11(4), 546–573. https://doi.org/10.1177/1745691616652873

Hardwicke, T. E., Serghiou, S., Janiaud, P., Danchev, V., Crüwell, S., Goodman, S. N., & Ioannidis, J. P. A. (2020). Calibrating the Scientific Ecosystem Through Meta-Research. Annual Review of Statistics and Its Application, 7(1), 11–37. https://doi.org/10.1146/annurev-statistics-031219-041104

Hardwicke, T. E., Thibault, R. T., Kosie, J., Wallach, J. D., Kidwell, M. C., & Ioannidis, J. (2021). Estimating the prevalence of transparency and reproducibility-related research practices in psychology (2014-2017). Perspectives on Psychological Science. https://doi.org/10.31222/osf.io/9sz2y

Ioannidis, J. P. A. (2012). Why science is not necessarily self-correcting. Perspectives on Psychological Science, 7(6), 645–654. https://doi.org/10.1177/1745691612464056

Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., Bocian, K., Brandt, M. J., Brooks, B., Brumbaugh, C. C., Cemalcilar, Z., Chandler, J., Cheong, W., Davis, W. E., Devos, T., Eisner, M., Frankowska, N., Furrow, D., Galliani, E. M., … Nosek, B. A. (2014). Investigating variation in replicability. Social Psychology, 45(3), 142–152. https://doi.org/10.1027/1864-9335/a000178

Kühl, T., & Bertrams, A. (2019). Is learning with elaborative interrogation less desirable when learners are depleted? Frontiers in Psychology, 10, 707. https://doi.org/10.3389/fpsyg.2019.00707

Kunda, Z. (1990). The case for motivated reasoning. Psychological Bulletin, 108(3), 480–498. https://doi.org/10.1037/0033-2909.108.3.480

Kvarven, A., Strømland, E., & Johannesson, M. (2020). Comparing meta-analyses and preregistered multiple-laboratory replication projects. Nature Human Behaviour, 4(4), 423–434. https://doi.org/10.1038/s41562-019-0787-z

Lakatos, I. (1970). Falsification and the methodology of scientific research programmes. In A. Musgrave & I. Lakatos (Eds.), Criticism and the Growth of Knowledge (Vol. 4, pp. 91–196). Cambridge University Press. https://doi.org/10.1017/CBO9781139171434.009

Lauden, L. (1981). Peirce and the trivialization of the self-corrective thesis. In Science and Hypothesis (pp. 226–251). Springer.

Lewandowsky, S., Ecker, U. K. H., Seifert, C. M., Schwarz, N., & Cook, J. (2012). Misinformation and its correction: Continued influence and successful debiasing. Psychological Science in the Public Interest, 13(3), 106–131. https://doi.org/10.1177/1529100612451018

Maassen, E., van Assen, M. A. L. M., Nuijten, M. B., Olsson-Collentine, A., & Wicherts, J. M. (2020). Reproducibility of individual effect sizes in meta-analyses in psychology. PLOS ONE, 15(5), e0233107. https://doi.org/10.1371/journal.pone.0233107

Maxwell, S. E., Lau, M. Y., & Howard, G. S. (2015). Is psychology suffering from a replication crisis? What does “failure to replicate” really mean? American Psychologist, 70(6), 487–498. https://doi.org/10.1037/a0039400

McDiarmid, A., Tullett, A., Whitt, C. M., Vazire, S., Smaldino, P. E., & Stephens, E. E. (2021). Self-correction in psychological science: How do psychologists update their beliefs in response to replications? PsyArXiv. https://doi.org/10.31234/osf.io/hjcm4

Nelson, L. D., Simmons, J., & Simonsohn, U. (2018). Psychology’s Renaissance. Annual Review of Psychology, 69(1), 511–534. https://doi.org/10.1146/annurev-psych-122216-011836

Neuliep, J. W., & Crandall, R. (1993). Everyone was wrong: There are lots of replications out there. Journal of Social Behavior and Personality, 8(6), 1–8.

Nickerson, R. S. (1998). Confirmation Bias: A Ubiquitous Phenomenon in Many Guises. Review of General Psychology, 2(2), 175–220. https://doi.org/10.1037/1089-2680.2.2.175

Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716–aac4716. https://doi.org/10.1126/science.aac4716

Peterson, D., & Panofsky, A. (2020). Self-correction in science: The diagnostic and integrative motives for replication. SocArXiv. https://doi.org/10.31235/osf.io/96qxv

Polanin, J. R., Hennessy, E. A., & Tsuji, S. (2020). Transparency and reproducibility of meta-analyses in psychology: A meta-review. Perspectives on Psychological Science, 15(4), 1026–1041. https://doi.org/10.1177/1745691620906416

Rohrer, D., Pashler, H., & Harris, C. R. (2015). Do subtle reminders of money change people’s political views? Journal of Experimental Psychology: General, 144(4), e73–e85. https://doi.org/10.1037/xge0000058

Simons, D. J., Holcombe, A. O., & Spellman, B. A. (2014). An introduction to Registered Replication Reports at Perspectives on Psychological Science. Perspectives on Psychological Science, 9(5), 552–555. https://doi.org/10.1177/1745691614543974

Sripada, C., Kessler, D., & Jonides, J. (2014). Methylphenidate blocks effort-induced depletion of regulatory control in healthy volunteers. Psychological Science, 25(6), 1227–1234. https://doi.org/10.1177/0956797614526415

Strack, F. (2017). From data to truth in psychological science. A personal perspective. Frontiers in Psychology, 8, 702. https://doi.org/10.3389/fpsyg.2017.00702

Strack, F., Martin, L. L., & Stepper, S. (1988). Inhibiting and facilitating conditions of the human smile: A nonobtrusive test of the Facial Feedback Hypothesis. Journal of Personality and Social Psychology, 54(5), 768–777. https://doi.org/10.1037/0022-3514.54.5.768

Tatsioni, A., Bonitsis, N. G., & Ioannidis, J. P. A. (2007). Persistence of contradicted claims in the literature. JAMA, 298(21), 2517–2526. https://doi.org/10.1001/jama.298.21.2517

Vadillo, M. A. (2019). Ego depletion may disappear by 2020. Social Psychology, 50(5–6), 282–291. https://doi.org/10.1027/1864-9335/a000375

Vadillo, M. A., Hardwicke, T. E., & Shanks, D. R. (2016). Selection bias, vote counting, and money-priming effects: A comment on Rohrer, Pashler, and Harris (2015) and Vohs (2015). Journal of Experimental Psychology: General, 145(5), 655–663. https://doi.org/10.1037/xge0000157

Vazire, S., & Holcombe, A. O. (2020). Where are the self-correcting mechanisms in science? PsyArXiv. https://doi.org/10.31234/osf.io/kgqzt

Wagenmakers, E.-J., Beek, T., Dijkhoff, L., Gronau, Q. F., Acosta, A., Adams, R. B., Albohn, D. N., Allard, E. S., Benning, S. D., Blouin-Hudon, E.-M., Bulnes, L. C., Caldwell, T. L., Calin-Jageman, R. J., Capaldi, C. A., Carfagno, N. S., Chasten, K. T., Cleeremans, A., Connell, L., DeCicco, J. M., … Zwaan, R. A. (2016). Registered Replication Report: Strack, Martin, & Stepper (1988). Perspectives on Psychological Science, 11(6), 917–928. https://doi.org/10.1177/1745691616674458

Xie, Y. (2017). Dynamic documents with R and knitr. CRC Press.

Zotero. (2019, June 14). Retracted item notifications with retraction watch integration. https://www.zotero.org/blog/retracted-item-notifications

Zwaan, R. A., Etz, A., Lucas, R. E., & Donnellan, M. B. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41, e120. https://doi.org/10.1017/S0140525X17001972

\newpage

```{r render_appendix, include=FALSE}
render_appendix("appendix.Rmd")
```
