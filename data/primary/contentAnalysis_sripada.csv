firstCoder,secondCoder,doi,authors,pubYear,excluded,exclusionReason,articleType,citesReplication,citationClassificationOriginal,citationClassificationAgreed,counterArguments,evidenceCounter,evidenceVerbatim,methodsCounter,methodsVerbatim,expertiseCounter,expertiseVerbatim,UT
TH,DS,10.1177/1088868318762183,"Friese, M; Loschelder, DD; Gieseler, K; Frankenbach, J; Inzlicht, M",2019,FALSE,"NA",No empirical data,TRUE,unfavourable,unfavourable,TRUE,NA,NA,TRUE,"1) With respect to the RRR (Hagger et al., 2016), criticsargue that the study protocol did not use prototypical operationalizationsof central variables. Specifically, the egodepletion manipulation (a variant of the widely used e-crossingtask; Baumeister et al., 1998) lacked a habit-creating firstphase that may be important to increase self-control demandsin a second phase (Baumeister & Vohs, 2016a). Indeed, thee-crossing task without habit-creating phase has been rarelyused in previous research, although paper–pencil variantshave produced expected depletion effects on self-controlperformance in a few studies (Tyler & Burns, 2009; Wan &Sternthal, 2008). Another potentially important deviationfrom previous research was that the e-crossing task was computerized,although most applications of this manipulationhave relied on the paper–pencil version. That said, recentevidence suggests that the e-crossing task can indeed lead todecrements in performance over time (within the same task),even without a prior habit-formation phase and even if thetask is computer administered, at least in variants somewhatlonger than the one used in the RRR (Arber et al., 2017).",NA,NA,WOS:000463615200001
TH,DS,10.1371/journal.pone.0213026,"Radel, R; Gruet, M; Barzykowski, K",2019,FALSE,"NA",Empirical data - laboratory study,TRUE,equivocal,unfavourable,FALSE,NA,NA,NA,NA,NA,NA,WOS:000460638800033
TH,DS,10.1080/01973533.2018.1530671,"Singh, RK; Goritz, AS",2019,FALSE,"NA",Empirical data - laboratory study,TRUE,equivocal,equivocal,TRUE,NA,NA,TRUE,"With regard to the failed replicationeffort, from using another e-crossing task adaptionour two single studies cannot be compared to the severalstudies conducted within the replication effort (Hagger et al., 2016). However, we feel that our findingsoffer a perspective of why the replication mayhave failed provided that ego depletion exists.       This suggests that discarding ego depletion as aconcept may be premature. Yet this does not change thefact that in ego depletion literature and research, thereare theoretical and methodological gray areas. Forexample, one such area pertains to which exact processesego depletion affects and which it does not affect.We have argued that acting against established behavioralpatterns is a demand that probably fosters egodepletion. Indeed, the studies of the replication effort(Hagger et al., 2016) found no effect when their taskneglected this potential cause of ego depletion.",NA,NA,WOS:000461576700001
TH,DS,10.1037/npe0000082,"Balafoutas, L; Kerschbamer, R; Oexl, R",2018,FALSE,"NA",Empirical data - laboratory study,TRUE,unfavourable,unfavourable,FALSE,NA,NA,NA,NA,NA,NA,WOS:000442587700003
TH,DS,10.1098/rsos.180390,"Vadillo, MA; Gold, N; Osman, M",2018,FALSE,"NA",Empirical data - commentary including analysis,TRUE,unfavourable,unfavourable,TRUE,NA,NA,TRUE,"Commenting on this outcome, Baumeister & Vohs [11] argued that the RRR relied on a poorly chosen depleting task. Hagger et al. [10] presented participants with a series of words on a computer screen and asked them to press a button if they saw an ‘e’ unless the ‘e’ was next to or one letter away from another vowel. In the no depletion condition, a key was pressed whenever an ‘e’ was present. Baumeister & Vohs pointed out that in order to induce ego depletion, there would usually be a prior stage, carried out by all participants, which would establish a habitual, dominant response, so that the depletion condition would involve overcoming a prior impulse. Thus, it is unlikely that Hagger et al.'s depletion task would reduce performance on a second self-control task because participants were not all given a preliminary habit-forming task of pressing a computer key whenever they saw an ‘e’, which they would have to overcome in the depletion condition. In support of this interpretation, an independent reanalysis of the RRR data conducted by Dang [12] found that the ego depletion effect was in fact significant within a subset of participants who stated that they had invested more effort on the e-crossing task. However, it is not totally clear whether this feature of the study can account for failing to observe ego depletion in the RRR, given that other studies (including the seminal study on which the RRR was based [13]) have reported positive results without using a preliminary habit-forming period (see also [14]).",NA,NA,WOS:000443443000053
TH,DS,10.1371/journal.pone.0199554,"Wolff, W; Baumann, L; Englert, C",2018,FALSE,"NA",Empirical data - laboratory study,TRUE,equivocal,equivocal,TRUE,NA,NA,TRUE,"According to the strength model [1], participants from the depletion conditionshould have performed significantly worse in this secondary task compared to participantsfrom the control condition. However, the RRR failed to find a significant ego depletion effect.Hagger et al. [14] stress that the results do not imply that the ego depletion effect does notexist, but that it is highly necessary to identify the potential causes for these null-findings.",NA,NA,WOS:000436088700023
TH,DS,10.1007/s11572-017-9423-z,"Koi, P; Uusitalo, S; Tuominen, J",2018,FALSE,"NA",No empirical data,FALSE,favourable,favourable,NA,NA,NA,NA,NA,NA,NA,WOS:000442987000002
TH,DS,10.1016/j.socec.2018.04.009,"Achtziger, A; Alos-Ferrer, C; Wagner, AK",2018,FALSE,"NA",Empirical data - laboratory study,TRUE,unfavourable,unfavourable,TRUE,NA,NA,TRUE,"This presence or absence of a habit-forming component in the letter-crossing task, however, has been the cause of recent debate, with Baumeister and Vohs (2016) listing the lack of a habit-forming stage in the Hagger and Chatzisarantis (2016) multi-lab replication study as a reason to dismiss the null findings. Baumeister and Vohs comment that the replications study’s version of the letter-crossing task, which also did not start with a habit forming stage prior to the instigation of a new set of rules, was an essential methodological flaw that invalidated the non-effects on the outcome measure. “Without first instilling the habit, there is nothing to override. This may be a difficult cognitive judgment task, but no impulse is overridden, contrary to the nature of self-control tasks” (Baumeister and Vohs, 2016, p. 574). Although there have been theoretical justifications for the requirement of behavioral inhibition within the letter-crossing task (e.g., Baumeister and Vohs, 2016), there has yet to be independent empirical evidence to give credence to such justifications.",NA,NA,WOS:000432773100018
TH,DS,10.3389/fpsyg.2018.00711,"Myers, L; Downie, S; Taylor, G; Marrington, J; Tehan, G; Ireland, MJ",2018,FALSE,"NA",Empirical data - laboratory study,TRUE,equivocal,equivocal,FALSE,NA,NA,NA,NA,NA,NA,WOS:000432576700002
TH,DS,10.1017/S0140525X17001972,"Zwaan, RA; Etz, A; Lucas, RE; Donnellan, MB",2018,FALSE,"NA",No empirical data,TRUE,unfavourable,unfavourable,FALSE,NA,NA,NA,NA,NA,NA,WOS:000458790700035
TH,DS,10.1146/annurev-psych-122216-011845,"Shrout, Patrick E.; Rodgers, Joseph L.",2018,FALSE,"NA",No empirical data,TRUE,equivocal,equivocal,FALSE,NA,NA,NA,NA,NA,NA,WOS:000424143700021
TH,DS,10.1037/pspi0000099,"Savani, Krishna; Job, Veronika",2017,FALSE,"NA",Empirical data - laboratory study,TRUE,equivocal,equivocal,FALSE,NA,NA,NA,NA,NA,NA,WOS:000412445100005
TH,DS,10.1371/journal.pone.0174331,"Emmerling, Franziska; Martijn, Carolien; Alberts, Hugo J. E. M.; Thomson, Alix C.; David, Bastian; Kessler, Daniel; Schuhmann, Teresa; Sack, Alexander T.",2017,FALSE,"NA",Empirical data - laboratory study,TRUE,favourable,favourable,FALSE,NA,NA,NA,NA,NA,NA,WOS:000399175000013
TH,DS,10.3389/fpsyg.2017.00273,"Drummond, Aaron; Philipp, Michael C.",2017,FALSE,"NA",No empirical data,TRUE,unclassifiable,unclassifiable,FALSE,NA,NA,NA,NA,NA,NA,WOS:000394804500001
TH,DS,10.1037/npe0000047,"Alos-Ferrer, C; Hugelschafer, S; Li, JH",2015,FALSE,"NA",Empirical data - laboratory study,FALSE,favourable,favourable,NA,NA,NA,NA,NA,NA,NA,WOS:000365705900001
TH,DS,10.1111/spc3.12200,"Inzlicht, M; Berkman, E",2015,FALSE,"NA",No empirical data,FALSE,favourable,favourable,NA,NA,NA,NA,NA,NA,NA,WOS:000214703800001
TH,DS,10.3389/fpsyg.2015.01425,"Englert, C; Wolff, W",2015,FALSE,"NA",No empirical data,FALSE,favourable,favourable,NA,NA,NA,NA,NA,NA,NA,WOS:000362853300001
TH,DS,10.1002/hbm.22833,"Lebedev, AV; Lovden, M; Rosenthal, G; Feilding, A; Nutt, DJ; Carhart-Harris, RL",2015,FALSE,"NA",Empirical data - laboratory study,FALSE,favourable,favourable,NA,NA,NA,NA,NA,NA,NA,WOS:000357949900021
TH,DS,10.1080/00221309.2015.1063475,"Frane, A",2015,FALSE,"NA",Empirical data - field study,FALSE,unclassifiable,unclassifiable,NA,NA,NA,NA,NA,NA,NA,WOS:000359949600005
TH,DS,NA,"Dienstbier, RA",2015,TRUE,No access,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,WOS:000383890800023
SC,TH,10.1016/j.joep.2018.11.001,"Alos-Ferrer, Carlos; Ritschel, Alexander; Garcia-Segarra, Jaume; Achtziger, Anja",2019,FALSE,NA,Empirical data - laboratory study,TRUE,favourable,favourable,TRUE,NA,NA,TRUE,"1) Further discussion (Carter and McCullough, 2014; Hagger and Chatzisarantis, 2014) culminated in a registered replication report (Hagger et al., 2016), which failed to find significant effects. This has led to skepticism and thorough questioning of the evidence (see, e.g., Gissubel, Beiramar, & Freire, 2018). The results of Hagger et al. (2016) were contested by Baumeister and Vohs (2016) due to the implementation of the initial task. In this task, based on Baumeister et al. (1998), self-control resources are depleted by crossing out instances of the letter “e” according to a complex rule with several exceptions (compared with the control, where all instances are crossed out). Inhibiting the impulse to cross out the letter requires self-control. Hagger et al. (2016) used a computerized version of the task by Sripada et al. (2014) which did not include a so-called habituation phase. Baumeister and Vohs (2016) argued that the habituation phase is necessary to create the habit which is to be inhibited later.          2) For the RTV analyses (and only for those) we further excluded participants with RTVs two standard deviations away from the experiment’s mean (15 exclusions), yielding N = 222 and N = 224 for Experiment 1 and Experiment 2, respectively. The registered replication of Hagger et al. (2016) used a demanding exclusion rule by which all participants with an error rate of 20% or more in the “e” task were excluded from the analysis. This criterion is questionable, because if the manipulation works, participants in the depletion condition face a cognitively more demanding task than controls, and hence it is to be expected that the exclusion rule leads to a selection problem where significantly more participants are excluded in the depletion condition than in the control condition. Higher error rates for the depletion task are, simply put, part of the nature of the task. This was overwhelmingly the case in both of our (independent) samples. Surprisingly, Hagger et al. (2016) report that this only occurred in 5 of the 23 studies they include, raising reasonable doubts as to whether their manipulation (without habituation) actually successfully induced ego depletion. In view of this possible criticism, we analyzed our data both with the exclusion rule mentioned above and without it. However, we obtained qualitatively the same results in both cases, and hence report here the largest sample, adding only a summary of the analysis of the restricted sample at the end of the Results section.",NA,NA,WOS:000503325400003
SC,TH,10.1027/1864-9335/a000375,"Vadillo, Miguel A.",2019,FALSE,NA,Data synthesis,TRUE,favourable,equivocal,TRUE,NA,NA,TRUE,"1) In psychology and the behavioral sciences, large-scale replication studies have typically obtained smaller effects than the original experiments they were intended to replicate. In the famous Open Science Collaboration (2015), a massive multi-site study aimed at replicating 100 random studies from four prestigious psychology journals, the effect sizes of the replications were, on average, half the size of the original studies (see also Camerer et al., 2018; Hawkins et al., 2018). Similarly, Camerer et al. (2016) replicated 18 experiments in economics and found numerically smaller effect sizes in 14 of them, compared to the results of the original experiments. Although some (less than half) of the effects tested in Many Labs 1 (Klein et al., 2014) yielded larger effects than the originals, Many Labs 3 (Ebersole et al., 2016) found systematically smaller effect sizes in all cases, and so have all the Registered Replication Reports (RRR) published so far in Perspectives in Psychological Science (e.g., Hagger et al., 2016; O’Donnell et al., 2018; Wagenmakers et al., 2016). Many factors may explain why observed effect sizes tend to decline over time. In some cases, the decline may reflect a genuine change in the prevalence or strength of the effect. For instance, meta-analytic evidence that the effectiveness of cognitive behavioral therapy is falling has been attributed to a decrease in the placebo effect elicited by this type of therapy (Johnsen & Friborg, 2015). In the same vein, Dijksterhuis (2018) and Strack (2016) have argued that failures to replicate their findings in RRRs are possibly due to the fact that participants in these studies, usually psychology students, are now more likely to be familiar with the effects under study, as they have become common topics in undergraduate psychology courses (see also Rand, 2018, for a similar argument). It is also possible that the smaller effects reported in most replication attempts are due to the use of imperfect methods. For instance, it has been claimed that “replicators” may lack the necessary expertise or competence (Baumeister, 2016), that the procedures of multi-site studies may be less sensitive than those of the original studies (Baumeister & Vohs, 2016), and that the samples tested in replications may differ in crucial characteristics from the samples tested in the original studies (Gilbert, King, Pettigrew, & Wilson, 2016; Vohs, 2015).                        2)Between 2013 and 2015, Evan Carter and colleagues published a series of meta-analytic studies showing alarming signs of publication and reporting biases in the ego depletion literature (Carter, Kofler, Forster, & McCullough, 2015; Carter & McCullough, 2013, 2014). Several analyses converged to the conclusion that once corrected for bias, the average effect size of these studies might not be significantly different from zero (see also Yost, 2016). To dissipate all doubts about the reliability of these effects, Martin Hagger and Nikos Chatzisarantis coordinated a large-scale RRR collecting data from 23 independent laboratories which, disappointingly, failed to find convincing evidence of ego depletion (Hagger et al., 2016). Baumeister and Vohs (2016) argued that the null results could be attributable to the unfortunate choice of a low-demanding depleting task – a criticism supported by complementary analyses by Dang (2016) and Drummond and Philipp (2017) – and that no safe conclusion about the reliability of ego depletion effects could be drawn from a single RRR exploring just one among many alternative procedures. Following up on this criticism, Kathleen Vohs, Brandon Schmeichel, and Roy Baumeister have designed and coordinated a second RRR. Although while I write these lines the final results of this RRR are not formally published, according to a preliminary presentation at the 2018 Annual Convention of the Society for Personality and Social Psychology the study found a statistically significant, but tiny (d = 0.08), effect (Srivastava, 2018). While it is legitimate to see this result as a “successful” demonstration of ego depletion, in the sense that the effect is non-zero according to null hypothesis significance testing, it is also clear that this effect is remarkably smaller than the early demonstrations of the effect.",TRUE,"In psychology and the behavioral sciences, large-scale replication studies have typically obtained smaller effects than the original experiments they were intended to replicate. In the famous Open Science Collaboration (2015), a massive multi-site study aimed at replicating 100 random studies from four prestigious psychology journals, the effect sizes of the replications were, on average, half the size of the original studies (see also Camerer et al., 2018; Hawkins et al., 2018). Similarly, Camerer et al. (2016) replicated 18 experiments in economics and found numerically smaller effect sizes in 14 of them, compared to the results of the original experiments. Although some (less than half) of the effects tested in Many Labs 1 (Klein et al., 2014) yielded larger effects than the originals, Many Labs 3 (Ebersole et al., 2016) found systematically smaller effect sizes in all cases, and so have all the Registered Replication Reports (RRR) published so far in Perspectives in Psychological Science (e.g., Hagger et al., 2016; O’Donnell et al., 2018; Wagenmakers et al., 2016). Many factors may explain why observed effect sizes tend to decline over time. In some cases, the decline may reflect a genuine change in the prevalence or strength of the effect. For instance, meta-analytic evidence that the effectiveness of cognitive behavioral therapy is falling has been attributed to a decrease in the placebo effect elicited by this type of therapy (Johnsen & Friborg, 2015). In the same vein, Dijksterhuis (2018) and Strack (2016) have argued that failures to replicate their findings in RRRs are possibly due to the fact that participants in these studies, usually psychology students, are now more likely to be familiar with the effects under study, as they have become common topics in undergraduate psychology courses (see also Rand, 2018, for a similar argument). It is also possible that the smaller effects reported in most replication attempts are due to the use of imperfect methods. For instance, it has been claimed that “replicators” may lack the necessary expertise or competence (Baumeister, 2016), that the procedures of multi-site studies may be less sensitive than those of the original studies (Baumeister & Vohs, 2016), and that the samples tested in replications may differ in crucial characteristics from the samples tested in the original studies (Gilbert, King, Pettigrew, & Wilson, 2016; Vohs, 2015).",WOS:000495020100002
SC,TH,10.1027/1864-9335/a000365,"Wenzel, Mario; Lind, Marina; Rowland, Zarah; Zahn, Daniala; Kubiak, Thomas",2019,FALSE,NA,Empirical data - laboratory study,TRUE,favourable,favourable,TRUE,NA,NA,TRUE,"1) The replication project has been criticized for employing a first task that may not be depleting enough to yield an ego depletion effect (Baumeister & Vohs, 2016) and that the results were slightly moderated (but were still insignificant at each condition of the moderator) by English as the first language (Sripada, Kessler, & Jonides, 2016).              2)Lange and Eggert (2014) found significant performance differences in their self-control tasks; these differences could have been misattributed to the experimental manipulation, but instead, they were the result of baseline performance differences. This could also partially explain the substantial between-laboratory variance found in the multi-lab replication project (Hagger et al., 2016) for which all study centers used the same instructions and materials.         3) This design not only allows for a longer wash-out period, reducing possible effects due to practice or the experimental setup, but also tests for intraindividual moderators and mediators of the ego depletion effect within-subject by showing that changes of the level of an intraindividual moderator can explain individual self-control performance in the second task across the conditions. This is particularly important because one explanation for the null effects in the multilab replication project of ego depletion (Hagger et al., 2016) was that participants in the depletion condition found the first task frustrating but not fatiguing (Dang, 2017).",NA,NA,WOS:000495020100003
SC,TH,10.1027/1864-9335/a000393,"Wimmer, Marina C.; Dome, Lenard; Hancock, Peter J. B.; Wennekers, Thomas",2019,FALSE,NA,Empirical data - laboratory study,TRUE,favourable,favourable,TRUE,NA,NA,TRUE,"Third, a preregistered replication study involving 23 laboratories that used the letter cancellation task as ego depletion measure and the multi-source interference task as outcome measure of inhibitory control (Carter et al., 2015; Etherton et al., 2018; Hagger et al., 2016) raised further doubts about the strength of the effect. However, interestingly the replication study showed great variations in task performance in accuracy both between ego depleted and control participants and across different laboratories (15%–44% of participants performing < 80% correct) and a great range of effect sizes on response time differences between ego depleted and control participants (95% CI on Cohen’s d = 0.06, 0.36). Moreover, other studies using the letter cancellation task in single laboratory studies found positive ego depletion effects using different types of outcome tasks (Baumeister et al., 1998; Stripada, Kessler, & Jonides, 2014; Wan & Sternthal, 2008; see also Hagger et al., 2010 for an overview) perhaps prematurely dismissing the existence of the effect per se.",NA,NA,WOS:000495020100007
SC,TH,10.1002/per.2208,"Wenzel, Mario; Rowland, Zarah; Zahn, Daniela; Kubiak, Thomas",2019,FALSE,NA,Empirical data - laboratory study,TRUE,favourable,favourable,FALSE,NA,NA,NA,NA,NA,NA,WOS:000478650900003
SC,TH,"NA","Christensen, G.; Freese, J.; Miguel, E.",2019,TRUE,No access,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,WOS:000518823400013
